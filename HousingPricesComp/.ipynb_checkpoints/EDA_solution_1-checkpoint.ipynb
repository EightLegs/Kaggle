{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, cross_validate, RandomizedSearchCV, GridSearchCV, KFold\n",
    "\n",
    "import time\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew, kurtosis, boxcox #for some statistics\n",
    "from scipy.special import boxcox1p, inv_boxcox, inv_boxcox1p\n",
    "\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTimer():\n",
    "    # usage:\n",
    "    #with MyTimer():                            \n",
    "    #    rf.fit(X_train, y_train)\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.start = time.time()\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        end = time.time()\n",
    "        runtime = end - self.start\n",
    "        msg = 'The function took {time} seconds to complete'\n",
    "        print(msg.format(time=runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImputeData(all_data, numerical_input, col_to_impute):\n",
    "    Missing = all_data[numerical_input]\n",
    "    imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "    imputer.fit(Missing)\n",
    "    Xtrans = imputer.transform(Missing)\n",
    "    df_miss = pd.DataFrame(Xtrans,columns = Missing.columns)\n",
    "    all_data[col_to_impute] = df_miss[col_to_impute]\n",
    "    return (all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Input//train.csv')\n",
    "test = pd.read_csv('Input//test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "#ax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\n",
    "ax.scatter(x = train['GrLivArea'], y = np.log(train['SalePrice']))\n",
    "plt.ylabel('SalePrice', fontsize=13)\n",
    "plt.xlabel('GrLivArea', fontsize=13)\n",
    "\n",
    "#m, b = np.polyfit(train['GrLivArea'], train['SalePrice'], 1)\n",
    "m, b = np.polyfit(train['GrLivArea'], np.log(train['SalePrice']), 1)\n",
    "#m = slope, b=intercept\n",
    "plt.plot(train['GrLivArea'], m*train['GrLivArea'] + b)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape[1]\n",
    "#a = int(np.sqrt(train.shape[1]))\n",
    "a = 4\n",
    "b = int(train.shape[1]/4)\n",
    "r = int(train.shape[1]/a)\n",
    "c = int(train.shape[1]/b)\n",
    "i = 0\n",
    "fig, ax = plt.subplots(nrows=r, ncols=c, figsize=(20, 60))\n",
    "for row in ax:\n",
    "    for col in row:\n",
    "        #print(train.columns[i])\n",
    "        #print(train[train.columns[i]].dtype)\n",
    "        #col.plot()\n",
    "        #ax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\n",
    "        try:\n",
    "            #col.scatter(x = train[train.columns[i]], y = train['SalePrice'])\n",
    "            col.scatter(x = train[train.columns[i]], y = np.log(train['SalePrice']))\n",
    "            col.title.set_text(train.columns[i])\n",
    "            #col.title(train.columns[i])\n",
    "        except:\n",
    "            temp=1\n",
    "        #except Exception as e:\n",
    "        #    print(e.message, e.args)\n",
    "        finally:\n",
    "            temp=1\n",
    "        i = i + 1\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Работа с выбросами\n",
    "train.drop(train[(train['LotFrontage'] > 250)].index, inplace=True)\n",
    "train.drop(train[(train['LotArea'] > 1e5)].index, inplace=True)\n",
    "train.drop(train[(train['MasVnrArea'] > 1250)].index, inplace=True)\n",
    "train.drop(train[(train['BsmtFinSF2'] > 1250)].index, inplace=True)\n",
    "train.drop(train[(train['TotalBsmtSF'] > 2700)].index, inplace=True)\n",
    "train.drop(train[(train['1stFlrSF'] > 2700)].index, inplace=True)\n",
    "train.drop(train[(train['2ndFlrSF'] > 1750)].index, inplace=True)\n",
    "train.drop(train[(train['LowQualFinSF'] > 570)].index, inplace=True)\n",
    "train.drop(train[(train['GrLivArea'] > 3700)].index, inplace=True)\n",
    "train.drop(train[(train['GarageArea'] > 1200)].index, inplace=True)\n",
    "train.drop(train[(train['WoodDeckSF'] > 800)].index, inplace=True)\n",
    "train.drop(train[(train['OpenPorchSF'] > 400)].index, inplace=True)\n",
    "train.drop(train[(train['EnclosedPorch'] > 500)].index, inplace=True)\n",
    "train.drop(train[(train['3SsnPorch'] > 400)].index, inplace=True)\n",
    "train.drop(train[(train['ScreenPorch'] > 400)].index, inplace=True)\n",
    "train.drop(train[(train['MiscVal'] > 8e3)].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train['SalePrice'] , fit=norm)\n",
    "\n",
    "_skew = skew(train['SalePrice'])\n",
    "_kurtosis = kurtosis(train['SalePrice'])\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(train['SalePrice'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}, skew = {:.2f} kurtosis = {:.2f}\\n'.format(mu, sigma, _skew, _kurtosis))\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(train['SalePrice'], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam_l = 0.35\n",
    "train[\"SalePrice\"] = boxcox1p(train[\"SalePrice\"], lam_l) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 20)\n",
    "y1 = np.log(x)\n",
    "y2 = np.log1p(x)\n",
    "y3 = boxcox1p(x, 0.35)\n",
    "y4 = boxcox1p(x, 0.10)\n",
    "y5 = boxcox1p(x, 0.50)\n",
    "plt.plot(x,y1,label='log') \n",
    "plt.plot(x,y2,label='log1p') \n",
    "plt.plot(x,y3,label='boxcox .35') \n",
    "plt.plot(x,y4,label='boxcox .10') \n",
    "plt.plot(x,y5,label='boxcox .50') \n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "x = np.linspace(0, 100000)\n",
    "y1 = np.log(x)\n",
    "y2 = np.log1p(x)\n",
    "y3 = boxcox1p(x, 0.35)\n",
    "y4 = boxcox1p(x, 0.10)\n",
    "y5 = boxcox1p(x, 0.50)\n",
    "plt.plot(x,y1,label='log') \n",
    "plt.plot(x,y2,label='log1p') \n",
    "plt.plot(x,y3,label='boxcox .35') \n",
    "plt.plot(x,y4,label='boxcox .10') \n",
    "plt.plot(x,y5,label='boxcox .50') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to revert back use y = inv_boxcox1p(x, lam_l) => train[\"SalePrice\"] = inv_boxcox1p(train[\"SalePrice\"], lam_l)\n",
    "# think the underlying formula is this: # pred_y = np.power((y_box * lambda_) + 1, 1 / lambda_) - 1\n",
    "\n",
    "\n",
    "#Check the new distribution \n",
    "sns.distplot(train['SalePrice'] , fit=norm);\n",
    "\n",
    "_skew = skew(train['SalePrice'])\n",
    "_kurtosis = kurtosis(train['SalePrice'])\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(train['SalePrice'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}, skew = {:.2f} kurtosis = {:.2f}\\n'.format(mu, sigma, _skew, _kurtosis))\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(train['SalePrice'], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train[\"SalePrice\"] = inv_boxcox1p(train[\"SalePrice\"], lam_l)\n",
    "train_ID = train['Id']\n",
    "test_ID = test['Id']\n",
    "\n",
    "\n",
    "train.drop(\"Id\", axis = 1, inplace = True)\n",
    "test.drop(\"Id\", axis = 1, inplace = True)\n",
    "\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "y_train = train.SalePrice.values\n",
    "all_data = pd.concat((train, test)).reset_index(drop=True)\n",
    "all_data.drop(['SalePrice'], axis=1, inplace=True)\n",
    "print(\"all_data size is : {}\".format(all_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_na = (all_data.isnull().sum() / len(all_data)) * 100\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
    "missing_data.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_numerical = all_data.select_dtypes(include=np.number).columns.tolist()\n",
    "missing_data.index.values.tolist()\n",
    "missing_df = all_data[missing_data.index.values.tolist()]\n",
    "missing_numerical = missing_df.select_dtypes(include=np.number).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 12))\n",
    "plt.xticks(rotation='90')\n",
    "sns.barplot(x=all_data_na.index, y=all_data_na)\n",
    "plt.xlabel('Features', fontsize=15)\n",
    "plt.ylabel('Percent of missing values', fontsize=15)\n",
    "plt.title('Percent missing data by feature', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation map to see how features are correlated with each other and with SalePrice\n",
    "corrmat = train.corr(method='kendall')\n",
    "plt.subplots(figsize=(12,9))\n",
    "sns.heatmap(corrmat, vmax=0.9, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (corrmat['SalePrice'].sort_values(ascending=False)[:5], '\\n')\n",
    "print (corrmat['SalePrice'].sort_values(ascending=False)[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImputeToNone = [\"Alley\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"PoolQC\", \"Fence\", \"MiscFeature\"]\n",
    "for col in ImputeToNone:  \n",
    "    all_data[col].fillna(\"None\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat_all = all_data[all_numerical].corr(method='kendall')\n",
    "corrmat_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat2 = all_data[missing_numerical].corr(method='kendall')\n",
    "corrmat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = ImputeData(all_data, all_numerical, 'LotFrontage')\n",
    "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n",
    "    all_data[col] = all_data[col].fillna(0)\n",
    "all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\n",
    "all_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n",
    "all_data[\"MSZoning\"] = all_data.groupby(\"Neighborhood\")[\"MSZoning\"].transform(lambda x: x.fillna(x.mode()))\n",
    "all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n",
    "all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n",
    "all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n",
    "all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\n",
    "all_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\n",
    "all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n",
    "all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n",
    "\n",
    "all_data = all_data.drop(['Utilities'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check remaining missing values if any \n",
    "all_data_na = (all_data.isnull().sum() / len(all_data)) * 100\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
    "missing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(all_data.isnull(), cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSSubClass=The building class\n",
    "all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n",
    "\n",
    "#Changing OverallCond into a categorical variable\n",
    "all_data['OverallCond'] = all_data['OverallCond'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
    "        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n",
    "        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n",
    "        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond')\n",
    "# process columns, apply LabelEncoder to categorical features\n",
    "for c in cols:\n",
    "    lbl = LabelEncoder() \n",
    "    lbl.fit(list(all_data[c].values)) \n",
    "    all_data[c] = lbl.transform(list(all_data[c].values))\n",
    "\n",
    "# shape\n",
    "print('Shape all_data: {}'.format(all_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering add new features \n",
    "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
    "\n",
    "all_data['YrBltAndRemod'] = all_data['YearBuilt'] + all_data['YearRemodAdd'] # A-\n",
    "all_data['Total_sqr_footage'] = (all_data['BsmtFinSF1'] + all_data['BsmtFinSF2'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']) # B-\n",
    "#all_data['Total_Bathrooms'] = (all_data['FullBath'] + (0.5 * all_data['HalfBath']) + all_data['BsmtFullBath'] + (0.5 * all_data['BsmtHalfBath'])) # C-\n",
    "#all_data['Total_porch_sf'] = (all_data['OpenPorchSF'] + all_data['3SsnPorch'] + all_data['EnclosedPorch'] + all_data['ScreenPorch'] + all_data['WoodDeckSF']) # D-\n",
    "#all_data['haspool'] = all_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0) # E-\n",
    "#all_data['has2ndfloor'] = all_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0) # F-\n",
    "#all_data['hasgarage'] = all_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0) # G-\n",
    "#all_data['hasbsmt'] = all_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0) # H-\n",
    "#all_data['hasfireplace'] = all_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0) # I-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.dtypes[all_data.dtypes == \"object\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "\n",
    "# Check the skew of all numerical features\n",
    "skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "skewness.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness = skewness[abs(skewness) > 0.75]\n",
    "print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
    "\n",
    "from scipy.special import boxcox1p\n",
    "skewed_features = skewness.index\n",
    "\n",
    "lam_f = 0.15\n",
    "for feat in skewed_features:\n",
    "    #all_data[feat] += 1\n",
    "    all_data[feat] = boxcox1p(all_data[feat], lam_f)\n",
    "    \n",
    "    #all_data[skewed_features] = np.log1p(all_data[skewed_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.get_dummies(all_data)\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = corrmat['SalePrice'].sort_values(ascending=False)\n",
    "df_corr = correlations.to_frame()\n",
    "print(df_corr.query(\"abs(SalePrice) < 0.011\"))\n",
    "low_corr = df_corr.query(\"abs(SalePrice) < 0.011\").index.values.tolist()\n",
    "#print('dropping these columns for low correlation', low_corr)\n",
    "#for i in low_corr: \n",
    "#    all_data = all_data.drop([i], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to choose number of components, look at this chart. Reference: https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html\n",
    "\n",
    "pca = PCA().fit(all_data)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back to original splits (from train.csv and test.csv)\n",
    "train = all_data[:ntrain]\n",
    "test = all_data[ntrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Сохранение данных в файл\n",
    "save_data = 1\n",
    "if (save_data == 1):\n",
    "    df1 = train.copy()\n",
    "    df1['SalePrice'] = inv_boxcox1p(y_train, lam_l)\n",
    "    df1.insert(0, 'Id', list(train_ID), allow_duplicates=False)\n",
    "    df1.to_csv('Input//HousePricesTrain.csv', index=False)  \n",
    "    df2 = test.copy()\n",
    "    df2.insert(0, 'Id', list(test_ID), allow_duplicates=False)\n",
    "    df2.to_csv('Input//HousePricesTest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation map to see how features are correlated with each other and with SalePrice\n",
    "corrmat = train.corr(method='kendall')\n",
    "plt.subplots(figsize=(24,18))\n",
    "sns.heatmap(corrmat, vmax=0.9, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.hist(bins=20, figsize=(30,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.values and y_train are both log scaled so just need to take the square of the delta between them to calculate the error, then take the sqrt to get rmsle\n",
    "# but for now y_train is boxcox1p(), not log(). Use this to convert back: inv_boxcox1p(y_train, lam_l)\n",
    "n_folds=5 # was 5 => better score but twice as slow now\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    print(\"running rmsle_cv code\")\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values) # was 42\n",
    "    # other scores: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf)) # also r2\n",
    "    print(\"raw rmse scores for each fold:\", rmse)\n",
    "    return(rmse)\n",
    "\n",
    "def r2_cv(model):\n",
    "    print(\"running r2_cv code\")\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values) # was 42\n",
    "    # other scores: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    r2= cross_val_score(model, train.values, y_train, scoring=\"r2\", cv = kf) # also r2\n",
    "    print(\"raw r2 scores for each fold:\", r2)\n",
    "    return(r2)\n",
    "\n",
    "# used for another competition\n",
    "def mae_cv(model):\n",
    "    print(\"running mae_cv code\")\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values) # was 42\n",
    "    # other scores: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    mae = -cross_val_score(model, train.values, y_train, scoring=\"neg_mean_absolute_error\", cv = kf) # also r2\n",
    "    print(\"raw mae scores for each fold:\", mae)\n",
    "    return(mae)\n",
    "\n",
    "def all_cv(model, n_folds, cv):\n",
    "    print(\"running cross_validate\")\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values) # was 42\n",
    "    # other scores: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    scorers = {\n",
    "        'r2': 'r2',\n",
    "        'nmsle': 'neg_mean_squared_log_error', \n",
    "        'nmse': 'neg_mean_squared_error',\n",
    "        'mae': 'neg_mean_absolute_error'\n",
    "    }\n",
    "    scores = cross_validate(model, train.values, y_train, scoring=scorers,\n",
    "                           cv=kf, return_train_score=True)\n",
    "    return(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.mean())\n",
    "print(inv_boxcox1p(y_train, lam_l).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_scores(model):\n",
    "    score_mae = mae_cv(model)\n",
    "    print(\"\\n mae_cv score: {:.4f} ({:.4f})\\n\".format(score_mae.mean(), score_mae.std()))\n",
    "    score_rmsle = rmsle_cv(model)\n",
    "    print(\"\\n rmsle_cv score: {:.4f} ({:.4f})\\n\".format(score_rmsle.mean(), score_rmsle.std()))\n",
    "    score_r2 = r2_cv(model)\n",
    "    print(\"\\n r2_cv score: {:.4f} ({:.4f})\\n\".format(score_r2.mean(), score_r2.std()))\n",
    "    return (score_mae, score_rmsle, score_r2)\n",
    "\n",
    "# calculate all 3 at once, takes 1/3 the time as calc_scores\n",
    "def calc_all_scores(model, n_folds=5, cv=5):\n",
    "    scores = all_cv(model, n_folds, cv)\n",
    "    #scores['train_<scorer1_name>'']\n",
    "    #scores['test_<scorer1_name>'']\n",
    "    print(\"\\n mae_cv score: {:.4f} ({:.4f})\\n\".format( (-scores['test_mae']).mean(), scores['test_mae'].std() ))\n",
    "    print(\"\\n rmsle_cv score: {:.4f} ({:.4f})\\n\".format( (np.sqrt(-scores['test_nmse'])).mean(), scores['test_nmse'].std() ))\n",
    "    print(\"\\n r2_cv score: {:.4f} ({:.4f})\\n\".format( scores['test_r2'].mean(), scores['test_r2'].std() ))\n",
    "    return (scores)\n",
    "\n",
    "# useful when you can't decide on parameter setting from best_params_\n",
    "# result_details(grid_result,'mean_test_nmse',100)\n",
    "def result_details(grid_result,sorting='mean_test_nmse',cols=100):\n",
    "    param_df = pd.DataFrame.from_records(grid_result.cv_results_['params'])\n",
    "    param_df['mean_test_nmse'] = np.sqrt(-grid_result.cv_results_['mean_test_nmse'])\n",
    "    param_df['std_test_nmse'] = np.sqrt(grid_result.cv_results_['std_test_nmse'])\n",
    "    param_df['mean_test_mae'] = -grid_result.cv_results_['mean_test_mae']\n",
    "    param_df['std_test_mae'] = -grid_result.cv_results_['std_test_mae']\n",
    "    param_df['mean_test_r2'] = -grid_result.cv_results_['mean_test_r2']\n",
    "    param_df['std_test_r2'] = -grid_result.cv_results_['std_test_r2']\n",
    "    return param_df.sort_values(by=[sorting]).tail(cols)\n",
    "\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "def mae(y, y_pred):\n",
    "    return mean_absolute_error(y,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runGSCV(num_trials, features, y_values):\n",
    "    non_nested_scores = np.zeros(num_trials) # INCREASES BIAS\n",
    "    nested_scores = np.zeros(num_trials)\n",
    "    # Loop for each trial\n",
    "    for i in range(num_trials):\n",
    "        print(\"Running GridSearchCV:\")\n",
    "        with MyTimer():    \n",
    "            #grid_result = gsc.fit(train, y_train)  \n",
    "            grid_result = gsc.fit(features, y_values)  \n",
    "        non_nested_scores[i] = grid_result.best_score_\n",
    "        #if (competition == 'SR'):\n",
    "        print(\"Best mae %f using %s\" % ( -grid_result.best_score_, grid_result.best_params_))\n",
    "        #else:\n",
    "        print(\"Best rmse %f using %s\" % ( np.sqrt(-grid_result.best_score_), grid_result.best_params_))\n",
    "        \n",
    "        # nested/non-nested cross validation: https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html\n",
    "        with MyTimer():    \n",
    "            #nested_score = cross_val_score(gsc, X=train, y=y_train, cv=outer_cv, verbose=0).mean() \n",
    "            nested_score = cross_val_score(gsc, X=features, y=y_values, cv=outer_cv, verbose=0).mean() \n",
    "            # source code for cross_val_score is here: https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/model_selection/_validation.py#L137\n",
    "        #if (competition == 'SR'):\n",
    "        print(\"nested mae score from KFold %0.3f\" % -nested_score)\n",
    "        #else:\n",
    "        print(\"nested rmse score from KFold %0.3f\" % np.sqrt(-nested_score))\n",
    "        \n",
    "        nested_scores[i] = nested_score\n",
    "        print('grid_result',grid_result)\n",
    "        #if (competition == 'SR'):\n",
    "        print(\"mean scores: r2(%0.3f) mae(%0.3f) nmse(%0.3f) nmsle(%0.3f)\" % (grid_result.cv_results_['mean_test_r2'].mean(), -grid_result.cv_results_['mean_test_mae'].mean(),  np.sqrt(-grid_result.cv_results_['mean_test_nmse'].mean()), grid_result.cv_results_['mean_test_nmsle'].mean() ))\n",
    "        #print(\"mean scores: r2(%0.3f) nmse(%0.3f) mae(%0.3f)\" % (grid_result.cv_results_['mean_test_r2'].mean(), np.sqrt(-grid_result.cv_results_['mean_test_nmse'].mean()), grid_result.cv_results_['mean_test_mae'].mean()))\n",
    "    return grid_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(random_state=8, n_jobs =-1)\n",
    "\n",
    "parameters_grid = {\n",
    "'colsample_bytree':[0.4603],\n",
    "            'gamma':[0.0468],\n",
    "            'colsample_bylevel':[0.3],\n",
    "            'objective':['reg:squarederror'], # 'binary:logistic', 'reg:squarederror', 'rank:pairwise', None\n",
    "            'booster':['gbtree'], # 'gbtree', 'gblinear' or 'dart'\n",
    "            'learning_rate':[0.04],\n",
    "            'max_depth':[3],\n",
    "            'importance_type':['gain'], # 'gain', 'weight', 'cover', 'total_gain' or 'total_cover'\n",
    "            'min_child_weight':[1.7817],\n",
    "            'n_estimators':[1000],\n",
    "            'reg_alpha':[0.4],\n",
    "            'reg_lambda':[0.8571],\n",
    "            'subsample':[0.5],\n",
    "            'silent':[1],\n",
    "            'random_state':[7],\n",
    "            'scale_pos_weight':[1],\n",
    "            'eval_metric':['rmse'],\n",
    "            #'nthread ':[-1],\n",
    "            'nthread ':[-1],\n",
    "            'verbosity':[0] \n",
    "}\n",
    "\n",
    "scorers = {\n",
    "    'r2': 'r2',\n",
    "    'nmsle': 'neg_mean_squared_log_error', \n",
    "    'nmse': 'neg_mean_squared_error',\n",
    "    'mae': 'neg_mean_absolute_error'\n",
    "}\n",
    "\n",
    "# To be used within GridSearch \n",
    "inner_cv = KFold(n_splits=3, shuffle=True, random_state=None)\n",
    "\n",
    "# To be used in outer CV \n",
    "outer_cv = KFold(n_splits=3, shuffle=True, random_state=None)    \n",
    "\n",
    "#inner loop KFold example:\n",
    "gsc = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=parameters_grid,\n",
    "    #scoring='neg_mean_squared_error', # 'roc_auc', # or 'r2', etc\n",
    "    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n",
    "    #scoring='neg_mean_squared_error', # or look here for other choices \n",
    "    # https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    #cv=5,\n",
    "    cv=inner_cv, # this will use KFold splitting to change train/test/validation datasets randomly\n",
    "    verbose=0,\n",
    "    return_train_score=True, # keep the other scores\n",
    "    refit='nmse' # use this one for optimizing\n",
    ")\n",
    "\n",
    "grid_result = runGSCV(1, train, y_train)\n",
    "\n",
    "#rd = result_details(grid_result,'random_state',100)\n",
    "#rd[['random_state','alpha','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']].sort_values(by=['random_state','alpha'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = grid_result.best_estimator_.predict(test)\n",
    "\n",
    "sub_train = pd.DataFrame()\n",
    "sub_train['Id'] = test_ID\n",
    "sub_train['SalePrice'] = inv_boxcox1p(preds, lam_l)\n",
    "\n",
    "sub_train.to_csv('Output//XGBoost_solution.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
