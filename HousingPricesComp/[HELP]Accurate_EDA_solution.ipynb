{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import some necessary libraries\n",
    "import datetime\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt  # Matlab-style plotting\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n",
    "import time\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew, kurtosis, boxcox #for some statistics\n",
    "from scipy.special import boxcox1p, inv_boxcox, inv_boxcox1p\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n",
    "\n",
    "from subprocess import check_output\n",
    "#print(check_output([\"ls\", \"-rlt\", \"../StackedRegression\"]).decode(\"utf8\")) #check the files available in the directory\n",
    "#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\")) #check the files available in the directory\n",
    "\n",
    "StartTime = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTimer():\n",
    "    # usage:\n",
    "    #with MyTimer():                            \n",
    "    #    rf.fit(X_train, y_train)\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.start = time.time()\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        end = time.time()\n",
    "        runtime = end - self.start\n",
    "        msg = 'The function took {time} seconds to complete'\n",
    "        print(msg.format(time=runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "competition='SR' #StackedRegression\n",
    "\n",
    "train = pd.read_csv('Input//train.csv')\n",
    "test = pd.read_csv('Input//test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осуществляется поиск выбросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "#ax.scatter(x = train['GrLivArea'], y = train['SalePrice']\n",
    "ax.scatter(x = train['GrLivArea'], y = np.log(train['SalePrice']))\n",
    "plt.ylabel('SalePrice', fontsize=13)\n",
    "plt.xlabel('GrLivArea', fontsize=13)\n",
    "\n",
    "#m, b = np.polyfit(train['GrLivArea'], train['SalePrice'], 1)\n",
    "m, b = np.polyfit(train['GrLivArea'], np.log(train['SalePrice']), 1)\n",
    "#m = slope, b=intercept\n",
    "plt.plot(train['GrLivArea'], m*train['GrLivArea'] + b)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape[1]\n",
    "#a = int(np.sqrt(train.shape[1]))\n",
    "a = 4\n",
    "b = int(train.shape[1]/4)\n",
    "r = int(train.shape[1]/a)\n",
    "c = int(train.shape[1]/b)\n",
    "i = 0\n",
    "fig, ax = plt.subplots(nrows=r, ncols=c, figsize=(15, 60))\n",
    "for row in ax:\n",
    "    for col in row:\n",
    "        #print(train.columns[i])\n",
    "        #print(train[train.columns[i]].dtype)\n",
    "        #col.plot()\n",
    "        #ax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\n",
    "        try:\n",
    "            #col.scatter(x = train[train.columns[i]], y = train['SalePrice'])\n",
    "            col.scatter(x = train[train.columns[i]], y = np.log(train['SalePrice']))\n",
    "            col.title.set_text(train.columns[i])\n",
    "            #col.title(train.columns[i])\n",
    "        except:\n",
    "            temp=1\n",
    "        #except Exception as e:\n",
    "        #    print(e.message, e.args)\n",
    "        finally:\n",
    "            temp=1\n",
    "        i = i + 1\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting outliers\n",
    "\n",
    "#train = train.drop(train[(train['LotFrontage']>300) & (train['SalePrice']<400000)].index)\n",
    "#train = train.drop(train[(train['LotArea']>100000) & (train['SalePrice']<400000)].index)\n",
    "#train = train.drop(train[(train['BsmtFinSF1']>4000) & (train['SalePrice']<250000)].index)\n",
    "#train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<250000)].index)\n",
    "\n",
    "train = train.drop(train[(train['OverallQual']>9) & (train['SalePrice']<220000)].index)\n",
    "train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the 'Id' column\n",
    "train_ID = train['Id']\n",
    "test_ID = test['Id']\n",
    "\n",
    "train.drop(\"Id\", axis = 1, inplace = True)\n",
    "test.drop(\"Id\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the graphic again\n",
    "fig, ax = plt.subplots()\n",
    "#ax.scatter(train['GrLivArea'], train['SalePrice'])\n",
    "ax.scatter(train['GrLivArea'], np.log(train['SalePrice']))\n",
    "plt.ylabel('SalePrice', fontsize=13)\n",
    "plt.xlabel('GrLivArea', fontsize=13)\n",
    "#m, b = np.polyfit(train['GrLivArea'], train['SalePrice'], 1)\n",
    "m, b = np.polyfit(train['GrLivArea'], np.log(train['SalePrice']), 1)\n",
    "#m = slope, b=intercept\n",
    "plt.plot(train['GrLivArea'], m*train['GrLivArea'] + b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear \n",
    "x_data = train['GrLivArea']\n",
    "y_data = train['SalePrice']\n",
    "log_y_data = np.log(train['SalePrice'])\n",
    "\n",
    "curve_fit = np.polyfit(x_data, log_y_data, 1)\n",
    "print(curve_fit)\n",
    "y = np.exp(1.11618915e+01) * np.exp(5.70762509e-04*x_data)\n",
    "plt.plot(x_data, y_data, \"o\")\n",
    "plt.scatter(x_data, y,c='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear with log y\n",
    "y = np.exp(1.11618915e+01) * np.exp(5.70762509e-04*x_data)\n",
    "plt.plot(x_data, np.log(y_data), \"o\")\n",
    "plt.scatter(x_data, np.log(y),c='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomial\n",
    "x_data = train['GrLivArea']\n",
    "y_data = train['SalePrice']\n",
    "log_y_data = np.log(train['SalePrice'])\n",
    "curve_fit_gla = np.polyfit(x_data, y_data, 2)\n",
    "y = curve_fit_gla[2] + curve_fit_gla[1]*x_data + curve_fit_gla[0]*x_data**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_data, y_data, \"o\")\n",
    "plt.scatter(x_data, y,c='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomial with log y\n",
    "plt.plot(x_data, np.log(y_data), \"o\")\n",
    "plt.scatter(x_data, np.log(y),c='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fit looks like it may be better, will add a column to the dataset using this equation after it is merged (all_data)\n",
    "y = curve_fit_gla[2] + curve_fit_gla[1]x_data + curve_fit_gla[0]x_data**2\n",
    "x_data = train['GrLivArea']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изучим целевую переменную, в частности, ее распределение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train['SalePrice'] , fit=norm)\n",
    "\n",
    "_skew = skew(train['SalePrice'])\n",
    "_kurtosis = kurtosis(train['SalePrice'])\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(train['SalePrice'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}, skew = {:.2f} kurtosis = {:.2f}\\n'.format(mu, sigma, _skew, _kurtosis))\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(train['SalePrice'], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable is right skewed. As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tansformation of the target variable\n",
    "\n",
    "Default is to use log1p as this is included in the evaluation metric, rmsle Edit: also trying box-cox transform. The Box-Cox transform is given by:\n",
    "\n",
    "y = (x**lmbda - 1) / lmbda,  for lmbda > 0\n",
    "\n",
    "    log(x),                  for lmbda = 0\n",
    "\n",
    "y = ((1+x)**lmbda - 1) / lmbda  if lmbda != 0\n",
    "    log(1+x)                    if lmbda == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We try the numpy function log1p which  applies log(1+x) to all elements of the column\n",
    "\n",
    "# option 1 - original\n",
    "#train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "\n",
    "# Option 2: use box-cox transform - this performs worse than the log(1+x), reverting back...now it seems to be scoring better. Maybe my earlier evaluation was incorrect\n",
    "# try different alpha values  between 0 and 1\n",
    "lam_l = 0.35\n",
    "train[\"SalePrice\"] = boxcox1p(train[\"SalePrice\"], lam_l) \n",
    "\n",
    "# Option 3: boxcox letting the algorithm select lmbda based on least-likelihood calculation\n",
    "#train[\"SalePrice\"], lam_l = boxcox1p(x=train[\"SalePrice\"], lmbda=None)\n",
    "\n",
    "# option 4 - compare to log1p => score is same\n",
    "#train[\"SalePrice\"] = np.log(train[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the difference between BoxCox and Log values, the difference is substantial at the value of a house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 20)\n",
    "y1 = np.log(x)\n",
    "y2 = np.log1p(x)\n",
    "y3 = boxcox1p(x, 0.35)\n",
    "y4 = boxcox1p(x, 0.10)\n",
    "y5 = boxcox1p(x, 0.50)\n",
    "plt.plot(x,y1,label='log') \n",
    "plt.plot(x,y2,label='log1p') \n",
    "plt.plot(x,y3,label='boxcox .35') \n",
    "plt.plot(x,y4,label='boxcox .10') \n",
    "plt.plot(x,y5,label='boxcox .50') \n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "x = np.linspace(0, 100000)\n",
    "y1 = np.log(x)\n",
    "y2 = np.log1p(x)\n",
    "y3 = boxcox1p(x, 0.35)\n",
    "y4 = boxcox1p(x, 0.10)\n",
    "y5 = boxcox1p(x, 0.50)\n",
    "plt.plot(x,y1,label='log') \n",
    "plt.plot(x,y2,label='log1p') \n",
    "plt.plot(x,y3,label='boxcox .35') \n",
    "plt.plot(x,y4,label='boxcox .10') \n",
    "plt.plot(x,y5,label='boxcox .50') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to revert back use y = inv_boxcox1p(x, lam_l) => train[\"SalePrice\"] = inv_boxcox1p(train[\"SalePrice\"], lam_l)\n",
    "# think the underlying formula is this: # pred_y = np.power((y_box * lambda_) + 1, 1 / lambda_) - 1\n",
    "\n",
    "\n",
    "#Check the new distribution \n",
    "sns.distplot(train['SalePrice'] , fit=norm);\n",
    "\n",
    "_skew = skew(train['SalePrice'])\n",
    "_kurtosis = kurtosis(train['SalePrice'])\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(train['SalePrice'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}, skew = {:.2f} kurtosis = {:.2f}\\n'.format(mu, sigma, _skew, _kurtosis))\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(train['SalePrice'], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skew seems now corrected and the data appears more normally distributed.\n",
    "\n",
    "Edit: Both distributions have a positive kurtosis, which means there is a steep dropoff in the curve from the center, or the tails have few points. Skewness is close to 0 now, so that metric is closer to a normal distribution. BoxCox1p() has worse numbers than log() but still performs better in the predictions, will keep BoxCox1p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работа с признаками\n",
    "\n",
    "Соединение датасетов, добавление признака кривой, которая рассматривалась ранее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "y_train = train.SalePrice.values\n",
    "all_data = pd.concat((train, test)).reset_index(drop=True)\n",
    "all_data.drop(['SalePrice'], axis=1, inplace=True)\n",
    "print(\"all_data size is : {}\".format(all_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gla(row, p):\n",
    "    return (p[2] + p[1]*row.GrLivArea + p[0]*(row.GrLivArea**2))\n",
    "\n",
    "#all_data['GrLivAreaPoly'] = all_data.apply(lambda row: add_gla(row,curve_fit_gla), axis=1)\n",
    "#all_data[['GrLivAreaPoly','GrLivArea']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save this inv function for later, may need it:\n",
    "import cmath\n",
    "d = (b2) - (4ac)\n",
    "print(-b-cmath.sqrt(d))/(2a)\n",
    "print(-b+cmath.sqrt(d))/(2a)\n",
    "def add_gla3(row, p):\n",
    "  return (p[2] + p[1]row.GrLivArea + p[0](row.GrLivArea2)) <-- change this function\n",
    "all_data['GrLivAreaPoly2'] = all_data.apply(lambda row: add_gla3(row,curve_fit_gla), axis=1)\n",
    "check and compare the new columns\n",
    "Now for r squared calculations we need to limit the comparison to only the train data since the test data does not have a Sales Price to compare to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = np.corrcoef(all_data.GrLivArea[:ntrain], y_train)\n",
    "correlation_xy = correlation_matrix[0,1]\n",
    "r_squared = correlation_xy**2\n",
    "\n",
    "print(r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the r-squared values for different functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,11,1):\n",
    "    j = i/10\n",
    "    correlation_matrix = np.corrcoef(all_data.GrLivArea[:ntrain]**j, y_train)\n",
    "    correlation_xy = correlation_matrix[0,1]\n",
    "    r_squared = correlation_xy**2\n",
    "\n",
    "    print(j, r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gla2(row, p):\n",
    "    return (row.GrLivArea**p)\n",
    "\n",
    "#all_data['GrLivAreaRoot'] = all_data.apply(lambda row: add_gla2(row,0.3), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работа с NA значениями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_na = (all_data.isnull().sum() / len(all_data)) * 100\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
    "missing_data.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_numerical = all_data.select_dtypes(include=np.number).columns.tolist()\n",
    "missing_data.index.values.tolist()\n",
    "missing_df = all_data[missing_data.index.values.tolist()]\n",
    "missing_numerical = missing_df.select_dtypes(include=np.number).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 12))\n",
    "plt.xticks(rotation='90')\n",
    "sns.barplot(x=all_data_na.index, y=all_data_na)\n",
    "plt.xlabel('Features', fontsize=15)\n",
    "plt.ylabel('Percent of missing values', fontsize=15)\n",
    "plt.title('Percent missing data by feature', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим корреляцию данных\n",
    "\n",
    "Cross Correlation chart to view collinearity within the features. Kendall's seems appropriate as the Output Variable is numerical and much of the input is categorical. Here is a chart to guide you to which method to use based on your dataset.\n",
    "\n",
    "(смотри листы со шпаргалками)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation map to see how features are correlated with each other and with SalePrice\n",
    "corrmat = train.corr(method='kendall')\n",
    "plt.subplots(figsize=(12,9))\n",
    "sns.heatmap(corrmat, vmax=0.9, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (corrmat['SalePrice'].sort_values(ascending=False)[:5], '\\n')\n",
    "print (corrmat['SalePrice'].sort_values(ascending=False)[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работа с заполнением пустых значений. Интересные ссылки и рассуждения\n",
    "\n",
    "Some imputation methods result in biased parameter estimates, such as means, correlations, and regression coefficients, unless the data are Missing Completely at Random (MCAR). The bias is often worse than with listwise deletion, the default in most software.\n",
    "\n",
    "The extent of the bias depends on many factors, including the imputation method, the missing data mechanism, the proportion of the data that is missing, and the information available in the data set.\n",
    "\n",
    "Moreover, all single imputation methods underestimate standard errors.\n",
    "\n",
    "Reference: https://www.theanalysisfactor.com/mar-and-mcar-missing-data/\n",
    "\n",
    "Multiple imputation can overcome most of these shortcomings, but at the expense of time. They take more time to implement and run\n",
    "\n",
    "Reference: https://www.theanalysisfactor.com/missing-data-two-recommended-solutions/\n",
    "\n",
    "Methods: (can use in pipeline - estimator = make_pipeline(imputer, regressor) ) reference: https://scikit-learn.org/stable/auto_examples/impute/plot_missing_values.html#sphx-glr-auto-examples-impute-plot-missing-values-py\n",
    "\n",
    "Simple Regression:\n",
    "\n",
    "from sklearn.impute import SimpleImputer imp = SimpleImputer(missing_values=np.nan, strategy='mean') for numerical imp = SimpleImputer(strategy=\"most_frequent\") for categorical\n",
    "\n",
    "Multivariate Regression: from sklearn.experimental import enable_iterative_imputer from sklearn.impute import IterativeImputer imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "\n",
    "kNN: from sklearn.impute import KNNImputer imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "\n",
    "Other: Stochastic regression, Bayesian Linear Regression, Bayesian Binary Logistic Regression https://pypi.org/project/autoimpute/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We impute them by proceeding sequentially through features with missing values\n",
    "\n",
    "PoolQC : data description says NA means \"No Pool\". That make sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general.\n",
    "MiscFeature : data description says NA means \"no misc feature\"\n",
    "Alley : data description says NA means \"no alley access\"\n",
    "Fence : data description says NA means \"no fence\"\n",
    "FireplaceQu : data description says NA means \"no fireplace\"\n",
    "GarageType, GarageFinish, GarageQual and GarageCond : Replacing missing data with None\n",
    "BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 : For all these categorical basement-related features, NaN means that there is no basement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImputeToNone = [\"Alley\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"PoolQC\", \"Fence\", \"MiscFeature\"]\n",
    "for col in ImputeToNone:  \n",
    "    all_data[col].fillna(\"None\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LotFrontage : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Можно не раскоменчивать, потому что в дальнейшем произойдет заполнение пустых значений методом к соседей\n",
    "#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\n",
    "#all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat_all = all_data[all_numerical].corr(method='kendall')\n",
    "corrmat_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat2 = all_data[missing_numerical].corr(method='kendall')\n",
    "corrmat2\n",
    "#print (corrmat2['LotFrontage'].sort_values(ascending=False), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImputeData(all_data, numerical_input, col_to_impute):\n",
    "    from sklearn.impute import KNNImputer\n",
    "    \n",
    "    Missing = all_data[numerical_input]\n",
    "    imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "    imputer.fit(Missing)\n",
    "    Xtrans = imputer.transform(Missing)\n",
    "    df_miss = pd.DataFrame(Xtrans,columns = Missing.columns)\n",
    "    all_data[col_to_impute] = df_miss[col_to_impute]\n",
    "    return (all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = ImputeData(all_data, all_numerical, 'LotFrontage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GarageYrBlt, GarageArea and GarageCars : Replacing missing data with 0 (Since No garage = no cars in such garage.)\n",
    "BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath : missing values are likely zero for having no basement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n",
    "    all_data[col] = all_data[col].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MasVnrArea and MasVnrType : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\n",
    "all_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSZoning (The general zoning classification) : 'RL' is by far the most common value. So we can fill in missing values with 'RL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['MSZoning'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternate Method to calculate missing Zoning values, neighborhood should be zoned the same most of the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one may be a bit dangerous, maybe try to get zone from neighborhood most common value, similar to LotFrontage previously\n",
    "all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n",
    "\n",
    "# NEW, slightly worse score\n",
    "#all_data[\"MSZoning\"] = all_data.groupby(\"Neighborhood\")[\"MSZoning\"].transform(\n",
    "#    lambda x: x.fillna(x.mode()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilities : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . Since the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling. We can then safely remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['Utilities'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.drop(['Utilities'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functional : data description says NA means typical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Electrical : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KitchenQual: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exterior1st and Exterior2nd : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\n",
    "all_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SaleType : Fill in again with most frequent which is \"WD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSSubClass : Na most likely means No building class. We can replace missing values with None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check remaining missing values if any \n",
    "all_data_na = (all_data.isnull().sum() / len(all_data)) * 100\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
    "missing_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Трансформация численных признаков, которые на самом деле категориальные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['MSSubClass'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['OverallCond'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавление признаков года и месяца"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "Yr = all_data['YrSold'].min()\n",
    "Mo = all_data['MoSold'].min()\n",
    "t = datetime.datetime(Yr, Mo, 1, 0, 0)\n",
    "\n",
    "def calculateYrMo (row):   \n",
    "    return int((datetime.datetime(row.YrSold,row.MoSold,1) - t).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either way will work\n",
    "#all_data['YrMoSold'] = all_data.apply(lambda row: int((datetime.datetime(row.YrSold,row.MoSold,1) - t).total_seconds()), axis=1)\n",
    "\n",
    "all_data['YrMoSold'] = all_data.apply(lambda row: calculateYrMo(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSSubClass=The building class\n",
    "all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n",
    "\n",
    "\n",
    "#Changing OverallCond into a categorical variable\n",
    "all_data['OverallCond'] = all_data['OverallCond'].astype(str)\n",
    "\n",
    "\n",
    "#Year and month sold are transformed into categorical features.\n",
    "all_data['YrSold'] = all_data['YrSold'].astype(str)\n",
    "all_data['MoSold'] = all_data['MoSold'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используется LabelEncoder, т.к. в данных признаках порядок имеет значение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "#cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
    "#        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n",
    "#        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n",
    "#        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n",
    "#        'YrSold', 'MoSold', 'YrMoSold')\n",
    "\n",
    "# Edit: Dropping PoolQC for missing values => makes the model worse, reverting\n",
    "#all_data = all_data.drop(['PoolQC'], axis=1)\n",
    "\n",
    "cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
    "        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n",
    "        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n",
    "        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n",
    "        'YrSold', 'MoSold', 'YrMoSold')\n",
    "# process columns, apply LabelEncoder to categorical features\n",
    "for c in cols:\n",
    "    lbl = LabelEncoder() \n",
    "    lbl.fit(list(all_data[c].values)) \n",
    "    all_data[c] = lbl.transform(list(all_data[c].values))\n",
    "\n",
    "# shape\n",
    "print('Shape all_data: {}'.format(all_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering add new features \n",
    "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
    "\n",
    "all_data['YrBltAndRemod'] = all_data['YearBuilt'] + all_data['YearRemodAdd'] # A-\n",
    "all_data['Total_sqr_footage'] = (all_data['BsmtFinSF1'] + all_data['BsmtFinSF2'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']) # B-\n",
    "#all_data['Total_Bathrooms'] = (all_data['FullBath'] + (0.5 * all_data['HalfBath']) + all_data['BsmtFullBath'] + (0.5 * all_data['BsmtHalfBath'])) # C-\n",
    "#all_data['Total_porch_sf'] = (all_data['OpenPorchSF'] + all_data['3SsnPorch'] + all_data['EnclosedPorch'] + all_data['ScreenPorch'] + all_data['WoodDeckSF']) # D-\n",
    "#all_data['haspool'] = all_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0) # E-\n",
    "#all_data['has2ndfloor'] = all_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0) # F-\n",
    "#all_data['hasgarage'] = all_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0) # G-\n",
    "#all_data['hasbsmt'] = all_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0) # H-\n",
    "#all_data['hasfireplace'] = all_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0) # I-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отклонение численных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "\n",
    "# Check the skew of all numerical features\n",
    "skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "skewness.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box Cox Transformation of (highly) skewed features\n",
    "\n",
    "We use the scipy function boxcox1p which computes the Box-Cox transformation of  1+x .\n",
    "\n",
    "Note that setting  λ=0  is equivalent to log1p used above for the target variable.\n",
    "\n",
    "See this page for more details on Box Cox Transformation as well as the scipy function's page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness = skewness[abs(skewness) > 0.75]\n",
    "print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
    "\n",
    "from scipy.special import boxcox1p\n",
    "skewed_features = skewness.index\n",
    "\n",
    "lam_f = 0.15\n",
    "for feat in skewed_features:\n",
    "    #all_data[feat] += 1\n",
    "    all_data[feat] = boxcox1p(all_data[feat], lam_f)\n",
    "    \n",
    "    #all_data[skewed_features] = np.log1p(all_data[skewed_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend plotting the cumulative sum of eigenvalues (assuming they are in descending order). If you divide each value by the total sum of eigenvalues prior to plotting, then your plot will show the fraction of total variance retained vs. number of eigenvalues. The plot will then provide a good indication of when you hit the point of diminishing returns (i.e., little variance is gained by retaining additional eigenvalues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OnehotEncode категориальные значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.get_dummies(all_data)\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = corrmat['SalePrice'].sort_values(ascending=False)\n",
    "df_corr = correlations.to_frame()\n",
    "print(df_corr.query(\"abs(SalePrice) < 0.011\"))\n",
    "low_corr = df_corr.query(\"abs(SalePrice) < 0.011\").index.values.tolist()\n",
    "#print('dropping these columns for low correlation', low_corr)\n",
    "#for i in low_corr: \n",
    "#    all_data = all_data.drop([i], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping even a single column gives a worse score Conclusion: there is information in every column and the models are able to extract that information effectively!\n",
    "\n",
    "Do some PCA for the dataset, to remove some of the collinearity. Not sure if this will have any effect as collinearity is usually not detrimental to many or most algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to choose number of components, look at this chart. Reference: https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA().fit(all_data)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose number of eigenvalues to calculate based on previous chart, 20 looks like a good number, the chart starts to roll off around 15 and almost hits max a 20. We can also try 30,40 and 50 to squeeze a little more variance out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do PCA/StandardScaler+clip here or before the skew boxcox1p transform\n",
    "\n",
    "n_components = 50\n",
    "pca = PCA(n_components=n_components)\n",
    "all_data_pca = pca.fit_transform(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_data.shape)\n",
    "print(all_data_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit: pca.components_ is the matrix you can use to calculate the inverse of the PCA analysis, i.e. go back to the original dataset reference: https://stats.stackexchange.com/a/143949"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.round(pca.components_, 3) \n",
    "ev = np.round(pca.explained_variance_ratio_, 3)\n",
    "print('explained variance ratio',ev)\n",
    "pca_wt = pd.DataFrame(weights)#, columns=all_data.columns)\n",
    "pca_wt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation map to see how features are correlated with each other and with SalePrice\n",
    "corrmat = pd.DataFrame(all_data_pca).corr(method='kendall')\n",
    "plt.subplots(figsize=(12,9))\n",
    "plt.title(\"Kendall's Correlation Matrix PCA applied\", fontsize=16)\n",
    "sns.heatmap(corrmat, vmax=0.9, square=True)\n",
    "\n",
    "\n",
    "#Correlation map to see how features are correlated with each other and with SalePrice\n",
    "corrmat = train.corr(method='kendall')\n",
    "plt.subplots(figsize=(12,9))\n",
    "plt.title(\"Kendall's Correlation Matrix Initial Train Set\", fontsize=16)\n",
    "sns.heatmap(corrmat, vmax=0.9, square=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the PCA analysis did its job, the features show little correlation now\n",
    "\n",
    "Another look at the feature to output correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_orig = train.copy()\n",
    "train_orig['SalePrice'] = y_train\n",
    "corrmat = train_orig.corr(method='kendall')\n",
    "print (corrmat['SalePrice'].sort_values(ascending=False)[:5], '\\n')\n",
    "print (corrmat['SalePrice'].sort_values(ascending=False)[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the new PCA dataset for analysis - RMSE looks works after PCA is applied, need to look at the Kaggle score later and see if it correlates, could be a mistake to use PCA on categorical dummy data. However, XGB is better with PCA n=50 option. Maybe use a heavier weight for that portion, or use all_data_pca only on that model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "rc = RobustScaler()\n",
    "\n",
    "use_pca = 0 # using PCA currently hurts the score\n",
    "use_normalization = 0 # using StandardScaler doesn't work, try RobustScaler now\n",
    "\n",
    "if (use_pca == 1):\n",
    "    all_data_pca = pd.DataFrame(all_data_pca)\n",
    "    train = all_data_pca[:ntrain]\n",
    "    test = all_data_pca[ntrain:]\n",
    "    all_data_pca.head()\n",
    "elif (use_normalization == 1):\n",
    "    train = all_data[:ntrain]\n",
    "    test = all_data[ntrain:]\n",
    "    scaled_features = sc.fit_transform(train.values)\n",
    "    train = pd.DataFrame(scaled_features, index=train.index, columns=train.columns)\n",
    "    scaled_features = sc.transform(test.values)\n",
    "    test = pd.DataFrame(scaled_features, index=test.index, columns=test.columns)   \n",
    "elif (use_normalization == 2):\n",
    "    train = all_data[:ntrain]\n",
    "    test = all_data[ntrain:]\n",
    "    scaled_features = rc.fit_transform(train.values)\n",
    "    train = pd.DataFrame(scaled_features, index=train.index, columns=train.columns)\n",
    "    scaled_features = rc.transform(test.values)\n",
    "    test = pd.DataFrame(scaled_features, index=test.index, columns=test.columns)  \n",
    "else:\n",
    "    # back to original splits (from train.csv and test.csv)\n",
    "    train = all_data[:ntrain]\n",
    "    test = all_data[ntrain:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция сохранения данных на диск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"save_data = 0\n",
    "if (save_data == 1):\n",
    "    df1 = train.copy()\n",
    "    df1['SalePrice'] = inv_boxcox1p(y_train, lam_l)\n",
    "    df1.insert(0, 'Id', list(train_ID), allow_duplicates=False)\n",
    "    df1.to_csv('HousePricesTrain.csv', index=False)  \n",
    "    df2 = test.copy()\n",
    "    df2.insert(0, 'Id', list(test_ID), allow_duplicates=False)\n",
    "    df2.to_csv('HousePricesTest.csv', index=False) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation map to see how features are correlated with each other and with SalePrice\n",
    "corrmat = train.corr(method='kendall')\n",
    "plt.subplots(figsize=(24,18))\n",
    "sns.heatmap(corrmat, vmax=0.9, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation map to see how features are correlated with each other and with SalePrice\n",
    "corrmat = test.corr(method='kendall')\n",
    "plt.subplots(figsize=(24,18))\n",
    "sns.heatmap(corrmat, vmax=0.9, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.hist(bins=20, figsize=(30,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.hist(bins=20, figsize=(30,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a='''\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "X = train\n",
    "y = y_train\n",
    "train, val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=201)#, stratify=y)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель и работа с ней"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC, LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split, cross_validate\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "#from sklearn.metrics import mean_squared_log_error\n",
    "# to run locally: conda install -c anaconda py-xgboost\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the cross_val_score function of Sklearn. However this function has no shuffle attribute, so we add one line of code, in order to shuffle the dataset prior to cross-validation\n",
    "\n",
    "replace cross_val_score() with cross_validate()\n",
    "\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "scoring = {'prec_macro': 'precision_macro',\n",
    "           'rec_macro': make_scorer(recall_score, average='macro')}\n",
    "scores = cross_validate(clf, X, y, scoring=scoring,\n",
    "                        cv=5, return_train_score=True)\n",
    "sorted(scores.keys())\n",
    "['fit_time', 'score_time', 'test_prec_macro', 'test_rec_macro',\n",
    " 'train_prec_macro', 'train_rec_macro']\n",
    "scores['train_rec_macro']\n",
    "array([0.97..., 0.97..., 0.99..., 0.98..., 0.98...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.values and y_train are both log scaled so just need to take the square of the delta between them to calculate the error, then take the sqrt to get rmsle\n",
    "# but for now y_train is boxcox1p(), not log(). Use this to convert back: inv_boxcox1p(y_train, lam_l)\n",
    "n_folds=5 # was 5 => better score but twice as slow now\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    print(\"running rmsle_cv code\")\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values) # was 42\n",
    "    # other scores: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf)) # also r2\n",
    "    print(\"raw rmse scores for each fold:\", rmse)\n",
    "    return(rmse)\n",
    "\n",
    "def r2_cv(model):\n",
    "    print(\"running r2_cv code\")\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values) # was 42\n",
    "    # other scores: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    r2= cross_val_score(model, train.values, y_train, scoring=\"r2\", cv = kf) # also r2\n",
    "    print(\"raw r2 scores for each fold:\", r2)\n",
    "    return(r2)\n",
    "\n",
    "# used for another competition\n",
    "def mae_cv(model):\n",
    "    print(\"running mae_cv code\")\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values) # was 42\n",
    "    # other scores: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    mae = -cross_val_score(model, train.values, y_train, scoring=\"neg_mean_absolute_error\", cv = kf) # also r2\n",
    "    print(\"raw mae scores for each fold:\", mae)\n",
    "    return(mae)\n",
    "\n",
    "def all_cv(model, n_folds, cv):\n",
    "    print(\"running cross_validate\")\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values) # was 42\n",
    "    # other scores: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    scorers = {\n",
    "        'r2': 'r2',\n",
    "        'nmsle': 'neg_mean_squared_log_error', \n",
    "        'nmse': 'neg_mean_squared_error',\n",
    "        'mae': 'neg_mean_absolute_error'\n",
    "    }\n",
    "    scores = cross_validate(model, train.values, y_train, scoring=scorers,\n",
    "                           cv=kf, return_train_score=True)\n",
    "    return(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.mean())\n",
    "print(inv_boxcox1p(y_train, lam_l).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Базовая модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runGSCV(num_trials, features, y_values):\n",
    "    non_nested_scores = np.zeros(num_trials) # INCREASES BIAS\n",
    "    nested_scores = np.zeros(num_trials)\n",
    "    # Loop for each trial\n",
    "    for i in range(num_trials):\n",
    "        print(\"Running GridSearchCV:\")\n",
    "        with MyTimer():    \n",
    "            #grid_result = gsc.fit(train, y_train)  \n",
    "            grid_result = gsc.fit(features, y_values)  \n",
    "        non_nested_scores[i] = grid_result.best_score_\n",
    "        if (competition == 'SR'):\n",
    "            print(\"Best mae %f using %s\" % ( -grid_result.best_score_, grid_result.best_params_))\n",
    "        else:\n",
    "            print(\"Best rmse %f using %s\" % ( np.sqrt(-grid_result.best_score_), grid_result.best_params_))\n",
    "        \n",
    "        # nested/non-nested cross validation: https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html\n",
    "        with MyTimer():    \n",
    "            #nested_score = cross_val_score(gsc, X=train, y=y_train, cv=outer_cv, verbose=0).mean() \n",
    "            nested_score = cross_val_score(gsc, X=features, y=y_values, cv=outer_cv, verbose=0).mean() \n",
    "            # source code for cross_val_score is here: https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/model_selection/_validation.py#L137\n",
    "        if (competition == 'SR'):\n",
    "            print(\"nested mae score from KFold %0.3f\" % -nested_score)\n",
    "        else:\n",
    "            print(\"nested rmse score from KFold %0.3f\" % np.sqrt(-nested_score))\n",
    "        \n",
    "        nested_scores[i] = nested_score\n",
    "        print('grid_result',grid_result)\n",
    "        #if (competition == 'SR'):\n",
    "        print(\"mean scores: r2(%0.3f) mae(%0.3f) nmse(%0.3f) nmsle(%0.3f)\" % (grid_result.cv_results_['mean_test_r2'].mean(), -grid_result.cv_results_['mean_test_mae'].mean(),  np.sqrt(-grid_result.cv_results_['mean_test_nmse'].mean()), grid_result.cv_results_['mean_test_nmsle'].mean() ))\n",
    "        #print(\"mean scores: r2(%0.3f) nmse(%0.3f) mae(%0.3f)\" % (grid_result.cv_results_['mean_test_r2'].mean(), np.sqrt(-grid_result.cv_results_['mean_test_nmse'].mean()), grid_result.cv_results_['mean_test_mae'].mean()))\n",
    "    return grid_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_scores(model):\n",
    "    score_mae = mae_cv(model)\n",
    "    print(\"\\n mae_cv score: {:.4f} ({:.4f})\\n\".format(score_mae.mean(), score_mae.std()))\n",
    "    score_rmsle = rmsle_cv(model)\n",
    "    print(\"\\n rmsle_cv score: {:.4f} ({:.4f})\\n\".format(score_rmsle.mean(), score_rmsle.std()))\n",
    "    score_r2 = r2_cv(model)\n",
    "    print(\"\\n r2_cv score: {:.4f} ({:.4f})\\n\".format(score_r2.mean(), score_r2.std()))\n",
    "    return (score_mae, score_rmsle, score_r2)\n",
    "\n",
    "# calculate all 3 at once, takes 1/3 the time as calc_scores\n",
    "def calc_all_scores(model, n_folds=5, cv=5):\n",
    "    scores = all_cv(model, n_folds, cv)\n",
    "    #scores['train_<scorer1_name>'']\n",
    "    #scores['test_<scorer1_name>'']\n",
    "    print(\"\\n mae_cv score: {:.4f} ({:.4f})\\n\".format( (-scores['test_mae']).mean(), scores['test_mae'].std() ))\n",
    "    print(\"\\n rmsle_cv score: {:.4f} ({:.4f})\\n\".format( (np.sqrt(-scores['test_nmse'])).mean(), scores['test_nmse'].std() ))\n",
    "    print(\"\\n r2_cv score: {:.4f} ({:.4f})\\n\".format( scores['test_r2'].mean(), scores['test_r2'].std() ))\n",
    "    return (scores)\n",
    "\n",
    "# useful when you can't decide on parameter setting from best_params_\n",
    "# result_details(grid_result,'mean_test_nmse',100)\n",
    "def result_details(grid_result,sorting='mean_test_nmse',cols=100):\n",
    "    param_df = pd.DataFrame.from_records(grid_result.cv_results_['params'])\n",
    "    param_df['mean_test_nmse'] = np.sqrt(-grid_result.cv_results_['mean_test_nmse'])\n",
    "    param_df['std_test_nmse'] = np.sqrt(grid_result.cv_results_['std_test_nmse'])\n",
    "    param_df['mean_test_mae'] = -grid_result.cv_results_['mean_test_mae']\n",
    "    param_df['std_test_mae'] = -grid_result.cv_results_['std_test_mae']\n",
    "    param_df['mean_test_r2'] = -grid_result.cv_results_['mean_test_r2']\n",
    "    param_df['std_test_r2'] = -grid_result.cv_results_['std_test_r2']\n",
    "    return param_df.sort_values(by=[sorting]).tail(cols)\n",
    "\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "def mae(y, y_pred):\n",
    "    return mean_absolute_error(y,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of possible scoring values:\n",
    "Regression\n",
    "\n",
    "‘explained_variance’ metrics.explained_variance_score\n",
    "‘max_error’ metrics.max_error\n",
    "‘neg_mean_absolute_error’ metrics.mean_absolute_error\n",
    "‘neg_mean_squared_error’ metrics.mean_squared_error\n",
    "‘neg_root_mean_squared_error’ metrics.mean_squared_error\n",
    "‘neg_mean_squared_log_error’ metrics.mean_squared_log_error\n",
    "‘neg_median_absolute_error’ metrics.median_absolute_error\n",
    "‘r2’ metrics.r2_score\n",
    "‘neg_mean_poisson_deviance’ metrics.mean_poisson_deviance\n",
    "‘neg_mean_gamma_deviance’ metrics.mean_gamma_deviance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO Regression :\n",
    "This model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline, also want to compare to StandardScaler() => RobustScaler() is slightly better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the algorithm for the GridSearchCV function\n",
    "lasso = Lasso()\n",
    "tuningLasso = 1 # takes 2 minutes to complete\n",
    "\n",
    "if (tuningLasso == 1):\n",
    "    # use this when tuning\n",
    "    param_grid={\n",
    "        'alpha':[0.01,], # done, lower keeps getting better, but don't want to go too low and begin overfitting (alpha is related to L1 reg)\n",
    "        'fit_intercept':[True], # done, big difference\n",
    "        'normalize':[False], # done, big difference\n",
    "        'precompute':[False], # done, no difference\n",
    "        'copy_X':[True], # done, no difference\n",
    "        'max_iter':[200], # done\n",
    "        'tol':[0.005], # done, not much difference\n",
    "        'warm_start':[False], # done, no difference\n",
    "        'positive':[False], # done, big difference\n",
    "        'random_state':[1],\n",
    "        'selection':['cyclic'] # done both are same, cyclic is default\n",
    "    }\n",
    "\n",
    "else:\n",
    "    # use this when not tuning\n",
    "    param_grid={\n",
    "        'alpha':[0.2],\n",
    "        'fit_intercept':[True],\n",
    "        'normalize':[False],\n",
    "        'precompute':[False],\n",
    "        'copy_X':[True],\n",
    "        'max_iter':[200],\n",
    "        'tol':[0.0001],\n",
    "        'warm_start':[False],\n",
    "        'positive':[False],\n",
    "        'random_state':[None],\n",
    "        'selection':['cyclic']\n",
    "    }\n",
    "\n",
    "scorers = {\n",
    "    'r2': 'r2',\n",
    "    'nmsle': 'neg_mean_squared_log_error', \n",
    "    'nmse': 'neg_mean_squared_error',\n",
    "    'mae': 'neg_mean_absolute_error'\n",
    "}\n",
    "# To be used within GridSearch \n",
    "inner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n",
    "\n",
    "# To be used in outer CV \n",
    "outer_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n",
    "\n",
    "#inner loop KFold example:\n",
    "gsc = GridSearchCV(\n",
    "    estimator=lasso,\n",
    "    param_grid=param_grid,\n",
    "    #scoring='neg_mean_squared_error', # 'roc_auc', # or 'r2', etc\n",
    "    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n",
    "    #scoring='neg_mean_squared_error', # or look here for other choices \n",
    "    # https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    #cv=5,\n",
    "    cv=inner_cv, # this will use KFold splitting to change train/test/validation datasets randomly\n",
    "    verbose=0,\n",
    "    return_train_score=True, # keep the other scores\n",
    "    refit='nmse' # use this one for optimizing\n",
    ")\n",
    "\n",
    "grid_result = runGSCV(2, train, y_train)\n",
    "\n",
    "rd = result_details(grid_result,'random_state',100)\n",
    "rd[['random_state','alpha','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']].sort_values(by=['random_state','alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_lasso = 1\n",
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1)) # was 1\n",
    "lasso_new = make_pipeline(RobustScaler(), Lasso(**grid_result.best_params_))\n",
    "#l = \"{'alpha': 0.2, 'copy_X': True, 'fit_intercept': True, 'max_iter': 200, 'normalize': False, 'positive': False, 'precompute': False, 'random_state': 1, 'selection': 'cyclic', 'tol': 0.005, 'warm_start': False}\"\n",
    "#Lasso_new = make_pipeline(RobustScaler(), Lasso(**l))\n",
    "#lasso_ss = make_pipeline(StandardScaler(), Lasso(alpha =0.0005, random_state=1)) # was 1 => worse score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (tuning_lasso == 1):\n",
    "    #TEMP\n",
    "    model_results = [] # model flow, mae, rmsle\n",
    "    models = [lasso, lasso_new]\n",
    "\n",
    "    for model in models:\n",
    "        #print(model)\n",
    "        with MyTimer(): \n",
    "            scores = calc_all_scores(model,5,5)\n",
    "        #print(\"------------------------------------------\")\n",
    "        model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n",
    "\n",
    "    df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n",
    "    df_mr.sort_values(by=['rmsle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1)) # was 1\n",
    "\n",
    "if (tuning_lasso == 1):\n",
    "    for i in [2,5,20,42,99]:\n",
    "        from sklearn.linear_model import Lasso\n",
    "        print('random_state =',i)\n",
    "\n",
    "        l = {'alpha': 0.01, 'copy_X': True, 'fit_intercept': True, 'max_iter': 200, 'normalize': False, 'positive': False, 'precompute': False, 'selection': 'cyclic', 'tol': 0.005, 'warm_start': False}\n",
    "        lasso_new = make_pipeline(RobustScaler(), Lasso(**l, random_state=i))\n",
    "        #lasso_new = Lasso(**l, random_state=i)\n",
    "\n",
    "        models = [lasso, lasso_new]\n",
    "\n",
    "        for model in models:\n",
    "            #print(model)\n",
    "            with MyTimer(): \n",
    "                scores = calc_all_scores(model,5,5)\n",
    "            #print(\"------------------------------------------\")\n",
    "            model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n",
    "\n",
    "        df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n",
    "        print(df_mr.sort_values(by=['rmsle']))\n",
    "else:\n",
    "    lasso_new = make_pipeline(RobustScaler(), Lasso(**grid_result.best_params_, random_state=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net Regression :\n",
    "again made robust to outliers\n",
    "\n",
    "combines Lasso L1 Linear regularization and Ridge L2 Quadratic/Squared regularization penalties together into one algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the algorithm for the GridSearchCV function\n",
    "ENet = ElasticNet()\n",
    "tuningENet = 0 # takes 2 minutes to complete\n",
    "\n",
    "if (tuningENet == 1):\n",
    "    # use this when tuning\n",
    "    param_grid={\n",
    "        'alpha':[0.005,0.01,0.05,0.1],\n",
    "        'l1_ratio':[.6,.65,.7,.75,.8,.85,.9],\n",
    "        'fit_intercept':[True], # ,False\n",
    "        'normalize':[False], # True,\n",
    "        'max_iter':range(50,500,50),\n",
    "        'selection':['random'], # 'cyclic',\n",
    "        'random_state':[None]\n",
    "    }\n",
    "\n",
    "else:\n",
    "    # use this when not tuning\n",
    "    param_grid={\n",
    "        'alpha':[0.05],\n",
    "        'l1_ratio':[.85],\n",
    "        'fit_intercept':[True],\n",
    "        'normalize':[False],\n",
    "        'max_iter':[100], # default 1000\n",
    "        'selection':['random']\n",
    "    }\n",
    "\n",
    "scorers = {\n",
    "    'r2': 'r2',\n",
    "    'nmsle': 'neg_mean_squared_log_error',\n",
    "    'nmse': 'neg_mean_squared_error',\n",
    "    'mae': 'neg_mean_absolute_error'\n",
    "}\n",
    "# To be used within GridSearch \n",
    "inner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n",
    "\n",
    "# To be used in outer CV \n",
    "outer_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n",
    "\n",
    "#inner loop KFold example:\n",
    "gsc = GridSearchCV(\n",
    "    estimator=ENet,\n",
    "    param_grid=param_grid,\n",
    "    #scoring='neg_mean_squared_error', # 'roc_auc', # or 'r2', etc\n",
    "    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n",
    "    #scoring='neg_mean_squared_error', # or look here for other choices \n",
    "    # https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    #cv=5,\n",
    "    cv=inner_cv, # this will use KFold splitting to change train/test/validation datasets randomly\n",
    "    verbose=0,\n",
    "    return_train_score=True, # keep the other scores\n",
    "    refit='nmse' # use this one for optimizing\n",
    ")\n",
    "\n",
    "grid_result = runGSCV(2, train, y_train)\n",
    "\n",
    "rd = result_details(grid_result,'mean_test_nmse',100)\n",
    "rd[['alpha','l1_ratio','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']]#.sort_values(by=['n_estimators','mean_test_nmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENet_orig = make_pipeline(StandardScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "ENet = make_pipeline(StandardScaler(), ElasticNet(**grid_result.best_params_, random_state=3))\n",
    "ENet_new = make_pipeline(RobustScaler(), ElasticNet(**grid_result.best_params_, random_state=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel Ridge Regression :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the algorithm for the GridSearchCV function\n",
    "KRR = KernelRidge()\n",
    "tuningKRR = 0 # this took 40 mins, 20 per iteration\n",
    "\n",
    "if (tuningKRR == 1):\n",
    "    # use this when tuning\n",
    "    param_grid={\n",
    "        'alpha':[2.0,2.2,2.4,2.6], \n",
    "        'kernel':['polynomial'], #for entire list see: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.kernel_metrics.html#sklearn.metrics.pairwise.kernel_metrics\n",
    "        'gamma':[0.0001,0.001,0.01,0.1],\n",
    "        'degree':[1,2,3,4,5,6], \n",
    "        'coef0':[0.1,0.3,0.5,1.0,2.0]\n",
    "    }\n",
    "\n",
    "else:\n",
    "    # use this when not tuning\n",
    "    # nmse: Best mae 583416973.611280 using {'alpha': 2.2, 'coef0': 0.5, 'degree': 5, 'gamma': 0.001, 'kernel': 'polynomial'}\n",
    "    # mae: Best mae 15805.764347 using {'alpha': 2.0, 'coef0': 0.1, 'degree': 5, 'gamma': 0.001, 'kernel': 'polynomial'}\n",
    "    param_grid={\n",
    "        'alpha':[2.2], \n",
    "        'kernel':['polynomial'], # 'linear', 'rbf'\n",
    "        'gamma':[0.001],\n",
    "        'degree':[5], \n",
    "        'coef0':[0.5]\n",
    "    }\n",
    "scorers = {\n",
    "    'r2': 'r2',\n",
    "    'nmsle': 'neg_mean_squared_log_error',\n",
    "    'nmse': 'neg_mean_squared_error',\n",
    "    'mae': 'neg_mean_absolute_error'\n",
    "}\n",
    "# To be used within GridSearch \n",
    "inner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n",
    "# To be used in outer CV \n",
    "outer_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n",
    "\n",
    "#inner loop KFold example:\n",
    "gsc = GridSearchCV(\n",
    "    estimator=KRR,\n",
    "    param_grid=param_grid,\n",
    "    #scoring='neg_mean_squared_error', # 'roc_auc', # or 'r2', etc\n",
    "    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n",
    "    #scoring='neg_mean_squared_error', # or look here for other choices \n",
    "    # https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    #cv=5,\n",
    "    cv=inner_cv, # this will use KFold splitting to change train/test/validation datasets randomly\n",
    "    verbose=0,\n",
    "    return_train_score=True, # keep the other scores\n",
    "    refit='nmse' # use this one for optimizing\n",
    ")\n",
    "\n",
    "grid_result = runGSCV(2, train, y_train)\n",
    "\n",
    "rd = result_details(grid_result,'mean_test_nmse',100)\n",
    "rd[['alpha','degree','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']]#.sort_values(by=['n_estimators','mean_test_nmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KRR_orig = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "KRR = KernelRidge(**grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Regressor :\n",
    "This model needs improvement, will run cross validate on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the algorithm for the GridSearchCV function\n",
    "rf = RandomForestRegressor()\n",
    "tuningRF = 0 # this took 2 hours last time, 1 hour per iteration\n",
    "\n",
    "if (tuningRF == 1):\n",
    "    # use this when tuning\n",
    "    param_grid={\n",
    "        'max_depth':[3,4,5],\n",
    "        'max_features':[None,'sqrt','log2'], \n",
    "        # 'max_features': range(50,401,50),\n",
    "        # 'max_features': [50,100], # can be list or range or other\n",
    "        'n_estimators':range(25,100,25), \n",
    "        #'class_weight':[None,'balanced'],  \n",
    "        'min_samples_leaf':range(5,15,5), \n",
    "        'min_samples_split':range(10,30,10), \n",
    "        'criterion':['mse', 'mae'] \n",
    "    }\n",
    "\n",
    "else:\n",
    "    # use this when not tuning\n",
    "    param_grid={\n",
    "        'max_depth':[5],\n",
    "        'max_features':[None], # max_features is None is default and works here, removing 'sqrt','log2'\n",
    "        # 'max_features': range(50,401,50),\n",
    "        # 'max_features': [50,100], # can be list or range or other\n",
    "        'n_estimators': [50], # number of trees selecting 100, removing range(50,126,25)\n",
    "        #'class_weight':[None], # None was selected, removing 'balanced'\n",
    "        'min_samples_leaf': [5], #selecting 10, removing range 10,40,10)\n",
    "        'min_samples_split': [10], # selecting 20, removing range(20,80,10),\n",
    "        'criterion':['mse'] # remove gini as it is never selected\n",
    "    }\n",
    "\n",
    "scorers = {\n",
    "    'r2': 'r2',\n",
    "    'nmsle': 'neg_mean_squared_log_error',\n",
    "    'nmse': 'neg_mean_squared_error',\n",
    "    'mae': 'neg_mean_absolute_error'\n",
    "}\n",
    "# To be used within GridSearch \n",
    "inner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n",
    "\n",
    "# To be used in outer CV \n",
    "outer_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n",
    "\n",
    "#inner loop KFold example:\n",
    "gsc = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    #scoring='neg_mean_squared_error', # 'roc_auc', # or 'r2', etc\n",
    "    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n",
    "    #scoring='neg_mean_squared_error', # or look here for other choices \n",
    "    # https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    #cv=5,\n",
    "    cv=inner_cv, # this will use KFold splitting to change train/test/validation datasets randomly\n",
    "    verbose=0,\n",
    "    return_train_score=True, # keep the other scores\n",
    "    refit='nmse' # use this one for optimizing\n",
    ")\n",
    "\n",
    "grid_result = runGSCV(2, train, y_train)\n",
    "\n",
    "rd = result_details(grid_result,'mean_test_nmse',100)\n",
    "rd[['n_estimators','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']]#.sort_values(by=['n_estimators','mean_test_nmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF_orig = make_pipeline(StandardScaler(), RandomForestRegressor(max_depth=3,n_estimators=500))\n",
    "RF = make_pipeline(StandardScaler(), RandomForestRegressor(**grid_result.best_params_)) # better than default, but still not good\n",
    "RF_new = make_pipeline(RobustScaler(), RandomForestRegressor(**grid_result.best_params_)) # better than default, but still not good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimize GBoost: \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression :\n",
    "With huber loss that makes it robust to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBoost_orig = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state=5) # was 5\n",
    "\n",
    "tuning_gb = 0\n",
    "if (tuning_gb == 1):\n",
    "    # initialize the algorithm for the GridSearchCV function\n",
    "    GBoost = GradientBoostingRegressor()\n",
    "    tuningGB = 0\n",
    "\n",
    "    if (tuningGB == 1):\n",
    "        # use this when tuning\n",
    "        param_grid={\n",
    "            #'loss':['ls','lad','huber','quantile'],\n",
    "            'loss':['huber'], # done\n",
    "            'learning_rate':[0.05],\n",
    "            'n_estimators':[2000], # done\n",
    "            'subsample':[1.0],\n",
    "            #'criterion':['friedman_mse','mse','mae'],\n",
    "            'criterion':['friedman_mse'], # done\n",
    "            'min_samples_split':[10],\n",
    "            'min_samples_leaf':[15],\n",
    "            'min_weight_fraction_leaf':[0.0],\n",
    "            'max_depth':[2], # done\n",
    "            'min_impurity_decrease':[0.0],\n",
    "            'min_impurity_split':[None],\n",
    "            'init':[None],\n",
    "            'random_state':[None],\n",
    "            #'max_features':[None,'auto','sqrt','log2'],\n",
    "            'max_features':['sqrt'], # done\n",
    "            'alpha':[0.60], # done\n",
    "            'verbose':[0],\n",
    "            'max_leaf_nodes':[None],\n",
    "            'warm_start':[False],\n",
    "            'presort':['deprecated'],\n",
    "            'validation_fraction':[0.1],\n",
    "            'n_iter_no_change':[None],\n",
    "            'tol':[0.0001],\n",
    "            'ccp_alpha':[0.0],\n",
    "            'random_state':[5]\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        # use this when not tuning\n",
    "        param_grid={\n",
    "            'loss':['huber'], \n",
    "            'learning_rate':[0.05],\n",
    "            'n_estimators':[2000], \n",
    "            'subsample':[1.0],\n",
    "            'criterion':['friedman_mse'], \n",
    "            'min_samples_split':[10],\n",
    "            'min_samples_leaf':[15],\n",
    "            'min_weight_fraction_leaf':[0.0],\n",
    "            'max_depth':[2], \n",
    "            'min_impurity_decrease':[0.0],\n",
    "            'min_impurity_split':[None],\n",
    "            'init':[None],\n",
    "            'random_state':[None],\n",
    "            'max_features':['sqrt'], \n",
    "            'alpha':[0.60], \n",
    "            'verbose':[0],\n",
    "            'max_leaf_nodes':[None],\n",
    "            'warm_start':[False],\n",
    "            'presort':['deprecated'],\n",
    "            'validation_fraction':[0.1],\n",
    "            'n_iter_no_change':[None],\n",
    "            'tol':[0.0001],\n",
    "            'ccp_alpha':[0.0],\n",
    "            'random_state':[5]\n",
    "        }\n",
    "    scorers = {\n",
    "        'r2': 'r2',\n",
    "        'nmsle': 'neg_mean_squared_log_error',\n",
    "        'nmse': 'neg_mean_squared_error',\n",
    "        'mae': 'neg_mean_absolute_error'\n",
    "    }\n",
    "    # To be used within GridSearch \n",
    "    inner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n",
    "    # To be used in outer CV \n",
    "    outer_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n",
    "\n",
    "    #inner loop KFold example:\n",
    "    gsc = GridSearchCV(\n",
    "        estimator=GBoost,\n",
    "        param_grid=param_grid,\n",
    "        scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n",
    "        cv=inner_cv, # this will use KFold splitting to change train/test/validation datasets randomly\n",
    "        verbose=0,\n",
    "        return_train_score=True, # keep the other scores\n",
    "        refit='nmse' # use this one for optimizing\n",
    "    )\n",
    "\n",
    "    grid_result = runGSCV(2, train, y_train)\n",
    "\n",
    "rd = result_details(grid_result,'mean_test_nmse',100)\n",
    "rd[['n_estimators','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']]#.sort_values(by=['n_estimators','mean_test_nmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am getting conflicting results for best params, sometimes huber or lad and sometimes friedman_mse or mae, so will look at more detailed output. This style output is much more useful for deciding between parameter values, adding the different random states shows the consistency, or lack of, for each setting\n",
    "\n",
    "maybe do a groupby to make this table more manageable and easier to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (tuning_gb == 1):\n",
    "    GBoost = GradientBoostingRegressor(**grid_result.best_params_)\n",
    "else:\n",
    "    gbr = gbr  = {'alpha': 0.6, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.05, 'loss': 'huber', 'max_depth': 2, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 15, 'min_samples_split': 10, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 2000, 'n_iter_no_change': None, 'presort': 'deprecated', 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
    "    GBoost = GradientBoostingRegressor(**gbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (tuning_gb == 1):\n",
    "    #TEMP\n",
    "    model_results = [] # model flow, mae, rmsle\n",
    "    models = [GBoost_orig, GBoost]\n",
    "\n",
    "    for model in models:\n",
    "        #print(model)\n",
    "        with MyTimer(): \n",
    "            scores = calc_all_scores(model,5,5)\n",
    "        #print(\"------------------------------------------\")\n",
    "        model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n",
    "\n",
    "    df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n",
    "    df_mr.sort_values(by=['rmsle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional testing\n",
    "Compare any random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (tuning_gb == 1):\n",
    "    #TEMP\n",
    "\n",
    "    # random_state=None\n",
    "    #new  = {'alpha': 0.6, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.05, 'loss': 'huber', 'max_depth': 2, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 15, 'min_samples_split': 10, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 2000, 'n_iter_no_change': None, 'presort': 'deprecated', 'random_state': 5, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
    "    #GBoost_new = GradientBoostingRegressor(**new)\n",
    "\n",
    "    model_results = [] # model flow, mae, rmsle\n",
    "    models = [GBoost_orig, GBoost]\n",
    "\n",
    "    for model in models:\n",
    "        #print(model)\n",
    "        with MyTimer(): \n",
    "            scores = calc_all_scores(model,10,10)\n",
    "        #print(\"------------------------------------------\")\n",
    "        model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n",
    "\n",
    "    df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n",
    "    df_mr.sort_values(by=['rmsle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimize XGB: \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_xgb = 0\n",
    "if (tuning_xgb == 1):\n",
    "    # initialize the algorithm for the GridSearchCV function# initialize the algorithm for the GridSearchCV function\n",
    "    xgb1 = xgb.XGBRegressor()\n",
    "    tuningXGB = 1 # this took 2 hours last time, 1 hour per iteration\n",
    "\n",
    "    if (tuningXGB == 1):\n",
    "        # use this when tuning\n",
    "        param_grid={\n",
    "            'colsample_bytree':[0.4603],\n",
    "            'gamma':[0.0468], # done - all values almost identical results\n",
    "            'colsample_bylevel':[0.3], # done - all give same result\n",
    "            'objective':['reg:squarederror'], # done - Default:'reg:squarederror', None, reg:pseudohubererror, reg:squaredlogerror, reg:gamma\n",
    "            'booster':['gbtree'], # done - Default: 'gbtree', 'gblinear' or 'dart'\n",
    "            'learning_rate':[0.04], # done\n",
    "            'max_depth':[3], # - done\n",
    "            'importance_type':['gain'], # done - all give same value, Default:'gain', 'weight', 'cover', 'total_gain' or 'total_cover'\n",
    "            'min_child_weight':[1.7817], # done - no difference with several values\n",
    "            'n_estimators':[1000], # done\n",
    "            'reg_alpha':[0.4], # done\n",
    "            'reg_lambda':[0.8571], # done\n",
    "            'subsample':[0.5], # done\n",
    "            'silent':[1],\n",
    "            'random_state':[7],\n",
    "            'scale_pos_weight':[1],\n",
    "            'eval_metric':['rmse'], # done - all options have same results  Default:rmse for regression rmse, mae, rmsle, logloss, cox-nloglik\n",
    "            #'nthread ':[-1],\n",
    "            'verbosity':[0]\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        # use this when not tuning\n",
    "        param_grid={\n",
    "            'colsample_bytree':[0.4603],\n",
    "            'gamma':[0.0468],\n",
    "            'colsample_bylevel':[0.3],\n",
    "            'objective':['reg:squarederror'], # 'binary:logistic', 'reg:squarederror', 'rank:pairwise', None\n",
    "            'booster':['gbtree'], # 'gbtree', 'gblinear' or 'dart'\n",
    "            'learning_rate':[0.04],\n",
    "            'max_depth':[3],\n",
    "            'importance_type':['gain'], # 'gain', 'weight', 'cover', 'total_gain' or 'total_cover'\n",
    "            'min_child_weight':[1.7817],\n",
    "            'n_estimators':[1000],\n",
    "            'reg_alpha':[0.4],\n",
    "            'reg_lambda':[0.8571],\n",
    "            'subsample':[0.5],\n",
    "            'silent':[1],\n",
    "            'random_state':[7],\n",
    "            'scale_pos_weight':[1],\n",
    "            'eval_metric':['rmse'],\n",
    "            #'nthread ':[-1],\n",
    "            'nthread ':[-1],\n",
    "            'verbosity':[0]\n",
    "        }\n",
    "\n",
    "    scorers = {\n",
    "        'r2': 'r2',\n",
    "        'nmsle': 'neg_mean_squared_log_error',\n",
    "        'nmse': 'neg_mean_squared_error',\n",
    "        'mae': 'neg_mean_absolute_error'\n",
    "    }\n",
    "    # To be used within GridSearch \n",
    "    inner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n",
    "\n",
    "    # To be used in outer CV \n",
    "    outer_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n",
    "\n",
    "    #inner loop KFold example:\n",
    "    gsc = GridSearchCV(\n",
    "        estimator=xgb1,\n",
    "        param_grid=param_grid,\n",
    "        #scoring='neg_mean_squared_error', # 'roc_auc', # or 'r2', etc\n",
    "        scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n",
    "        #scoring='neg_mean_squared_error', # or look here for other choices \n",
    "        # https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "        #cv=5,\n",
    "        cv=inner_cv, # this will use KFold splitting to change train/test/validation datasets randomly\n",
    "        verbose=0,\n",
    "        return_train_score=True, # keep the other scores\n",
    "        refit='nmse' # use this one for optimizing\n",
    "    )\n",
    "\n",
    "    grid_result = runGSCV(2, train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgb reference: https://xgboost.readthedocs.io/en/latest/parameter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (tuning_xgb == 1):\n",
    "    rd = result_details(grid_result,'mean_test_nmse',100)\n",
    "\n",
    "    rd[['random_state','eval_metric','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']].sort_values(by=['random_state','mean_test_nmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = xgb.XGBRegressor(booster='gbtree',colsample_bytree=0.4603, gamma=0.0468, # colsample_bylevel, objective, booster (gbtree, gblinear or dart.) # Default: 'gbtree'\n",
    "                             learning_rate=0.05, max_depth=3, # importance_type (“gain”, “weight”, “cover”, “total_gain” or “total_cover”.)\n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state=7) # was random_state=7, cannot set to None \n",
    "# maybe an issue with silent=1...\n",
    "\n",
    "if (tuning_xgb == 1):\n",
    "    import xgboost as xgb\n",
    "    for i in [2,5,20,42,99]:\n",
    "        print('random_state =',i)\n",
    "\n",
    "        model_xgb_new = xgb.XGBRegressor(booster='gbtree',colsample_bytree=0.4603, gamma=0.0468, # colsample_bylevel, objective, booster (gbtree, gblinear or dart.) # Default: 'gbtree'\n",
    "                                 learning_rate=0.04, max_depth=3, # importance_type (“gain”, “weight”, “cover”, “total_gain” or “total_cover”.)\n",
    "                                 min_child_weight=1.7817, n_estimators=1000,\n",
    "                                 reg_alpha=0.4, reg_lambda=0.8571,\n",
    "                                 subsample=0.45, silent=1,\n",
    "                                 random_state=i) # was random_state=7, cannot set to None \n",
    "\n",
    "        model_xgb_new2 = xgb.XGBRegressor(booster='gbtree',colsample_bytree=0.4603, gamma=0.0468, # colsample_bylevel, objective, booster (gbtree, gblinear or dart.) # Default: 'gbtree'\n",
    "                                 learning_rate=0.04, max_depth=3, # importance_type (“gain”, “weight”, “cover”, “total_gain” or “total_cover”.)\n",
    "                                 min_child_weight=1.7817, n_estimators=1000,\n",
    "                                 reg_alpha=0.4, reg_lambda=0.8571,\n",
    "                                 subsample=0.5, silent=1,\n",
    "                                 random_state=i) # was random_state=7, cannot set to None \n",
    "\n",
    "        model_xgb_new3 = xgb.XGBRegressor(booster='gbtree',colsample_bytree=0.4603, gamma=0.0468, # colsample_bylevel, objective, booster (gbtree, gblinear or dart.) # Default: 'gbtree'\n",
    "                                 learning_rate=0.04, max_depth=3, # importance_type (“gain”, “weight”, “cover”, “total_gain” or “total_cover”.)\n",
    "                                 min_child_weight=1.7817, n_estimators=1000,\n",
    "                                 reg_alpha=0.4, reg_lambda=0.8571,\n",
    "                                 subsample=0.5213, silent=1,\n",
    "                                 random_state=i) # was random_state=7, cannot set to None\n",
    "\n",
    "        model_results = [] # model flow, mae, rmsle\n",
    "        models = [model_xgb, model_xgb_new, model_xgb_new2, model_xgb_new3]\n",
    "\n",
    "        for model in models:\n",
    "            #print(model)\n",
    "            with MyTimer(): \n",
    "                scores = calc_all_scores(model,5,5)\n",
    "            #print(\"------------------------------------------\")\n",
    "            model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n",
    "\n",
    "        df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n",
    "        print(df_mr.sort_values(by=['rmsle']))\n",
    "else:\n",
    "    model_xgb_new = xgb.XGBRegressor(**grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "model_xgb.fit(train, y_train,  verbose=False) #  eval_set=[(X_test, y_test)]\n",
    "xgb.plot_importance(model_xgb)\n",
    "xgb.to_graphviz(model_xgb, num_trees=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimize LightGBM: \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_lgb = 0\n",
    "if (tuning_lgb == 1):\n",
    "    # initialize the algorithm for the GridSearchCV function\n",
    "    #model_lgb = lgb.LGBMRegressor(boosting_type='gbdt', max_depth=- 1, \n",
    "    #                            subsample_for_bin=200000, \n",
    "    #                            objective='regression',num_leaves=5,\n",
    "    #                            learning_rate=0.05, n_estimators=720,\n",
    "    #                            class_weight=None, min_split_gain=0.0, \n",
    "    #                            min_child_weight=0.001, min_child_samples=20, subsample=1.0, \n",
    "    #                            subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, \n",
    "    #                            random_state=None, n_jobs=- 1, silent=True, importance_type='split')\n",
    "\n",
    "    lgb1 = lgb.LGBMRegressor()\n",
    "    tuningLGB = 0\n",
    "\n",
    "    if (tuningLGB == 1):\n",
    "        # use this when tuning\n",
    "        param_grid={\n",
    "            'objective':['regression'], # - only one option for regression\n",
    "            'boosting_type':['gbdt'], # - done gbdt dart goss rf\n",
    "            'num_leaves':[5,6], # - done\n",
    "            'learning_rate':[0.05], # - done\n",
    "            'n_estimators':[650,750], # - done\n",
    "            'max_bin':[45,55], # - done\n",
    "            'bagging_fraction':[0.85], # - done\n",
    "            'bagging_freq':[5], # - done\n",
    "            'feature_fraction':[0.2319], # - done\n",
    "            'feature_fraction_seed':[9], \n",
    "            'bagging_seed':[9],\n",
    "            'min_data_in_leaf':[9], # - done\n",
    "            'min_sum_hessian_in_leaf':[11], # - done\n",
    "            'max_depth':[-1], # - -1 means no limit\n",
    "            'subsample_for_bin':[500,1000], # - done\n",
    "            'class_weight':[None],\n",
    "            'min_split_gain':[0.0],\n",
    "            'min_child_weight':[0.001],\n",
    "            'min_child_samples':[5], # - done\n",
    "            'subsample':[1.0],\n",
    "            'subsample_freq':[0],\n",
    "            'colsample_bytree':[1.0],\n",
    "            'reg_alpha':[0.0], # - l1 regularization done\n",
    "            'reg_lambda':[0.0], # - L2 regularization done\n",
    "            'random_state':[1],\n",
    "            'importance_type':['split'] # - done\n",
    "        }\n",
    "    else:\n",
    "        # use this when not tuning\n",
    "        param_grid={\n",
    "            'objective':['regression'], # - only one option for regression\n",
    "            'boosting_type':['gbdt'], # - done gbdt dart goss rf\n",
    "            'num_leaves':[5], # - done, maybe 5 is okay too\n",
    "            'learning_rate':[0.05], # - done\n",
    "            'n_estimators':[650], # - done\n",
    "            'max_bin':[55], # - done\n",
    "            'bagging_fraction':[0.85], # - done\n",
    "            'bagging_freq':[5], # - done\n",
    "            'feature_fraction':[0.2319], # - done\n",
    "            'feature_fraction_seed':[9], \n",
    "            'bagging_seed':[9],\n",
    "            'min_data_in_leaf':[9], # - done\n",
    "            'min_sum_hessian_in_leaf':[11], # - done\n",
    "            'max_depth':[-1], # - -1 means no limit\n",
    "            'subsample_for_bin':[1000], # - done\n",
    "            'class_weight':[None],\n",
    "            'min_split_gain':[0.0],\n",
    "            'min_child_weight':[0.001],\n",
    "            'min_child_samples':[5], # - done\n",
    "            'subsample':[1.0],\n",
    "            'subsample_freq':[0],\n",
    "            'colsample_bytree':[1.0],\n",
    "            'reg_alpha':[0.0], # - l1 regularization done\n",
    "            'reg_lambda':[0.0], # - L2 regularization done\n",
    "            'random_state':[1],\n",
    "            'importance_type':['split'] # - done\n",
    "        }\n",
    "\n",
    "    scorers = {\n",
    "        'r2': 'r2',\n",
    "        'nmsle': 'neg_mean_squared_log_error',\n",
    "        'nmse': 'neg_mean_squared_error',\n",
    "        'mae': 'neg_mean_absolute_error'\n",
    "    }\n",
    "    # To be used within GridSearch \n",
    "    inner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n",
    "\n",
    "    # To be used in outer CV \n",
    "    outer_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n",
    "\n",
    "    #inner loop KFold example:\n",
    "    gsc = GridSearchCV(\n",
    "        estimator=lgb1,\n",
    "        param_grid=param_grid,\n",
    "        #scoring='neg_mean_squared_error', # 'roc_auc', # or 'r2', etc\n",
    "        scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n",
    "        #scoring='neg_mean_squared_error', # or look here for other choices \n",
    "        # https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "        #cv=5,\n",
    "        cv=inner_cv, # this will use KFold splitting to change train/test/validation datasets randomly\n",
    "        verbose=0,\n",
    "        return_train_score=True, # keep the other scores\n",
    "        refit='nmse' # use this one for optimizing\n",
    "    )\n",
    "\n",
    "    grid_result = runGSCV(2, train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
    "if (tuning_lgb == 1):\n",
    "    model_lgb_op = lgb.LGBMRegressor(**grid_result.best_params_)\n",
    "else:\n",
    "    lgbm = {'bagging_fraction': 0.85, 'bagging_freq': 5, 'bagging_seed': 9, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'feature_fraction': 0.2319, 'feature_fraction_seed': 9, 'importance_type': 'split', 'learning_rate': 0.05, 'max_bin': 55, 'max_depth': -1, 'min_child_samples': 5, 'min_child_weight': 0.001, 'min_data_in_leaf': 9, 'min_split_gain': 0.0, 'min_sum_hessian_in_leaf': 11, 'n_estimators': 650, 'num_leaves': 5, 'objective': 'regression', 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 1000, 'subsample_freq': 0}\n",
    "    model_lgb_op = lgb.LGBMRegressor(**lgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BayesianRidge :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the algorithm for the GridSearchCV function\n",
    "br = BayesianRidge()\n",
    "tuningBR = 1 # this took 2 hours last time, 1 hour per iteration\n",
    "\n",
    "if (tuningBR == 1):\n",
    "    # use this when tuning\n",
    "    param_grid={\n",
    "        'n_iter':[50],\n",
    "        'tol':[0.001],\n",
    "        'alpha_1':[1e-06],\n",
    "        'alpha_2':[1e-05],\n",
    "        'lambda_1':[1e-05],\n",
    "        'lambda_2':[1e-06],\n",
    "        'alpha_init':[None],\n",
    "        'lambda_init':[None],\n",
    "        'compute_score':[True,False],\n",
    "        'fit_intercept':[True,False],\n",
    "        'normalize':[False,True],\n",
    "        'copy_X':[True],\n",
    "        'verbose':[False]\n",
    "    }\n",
    "\n",
    "else:\n",
    "    # use this when not tuning\n",
    "    param_grid={\n",
    "        'n_iter':[50],\n",
    "        'tol':[0.001],\n",
    "        'alpha_1':[1e-06],\n",
    "        'alpha_2':[1e-05],\n",
    "        'lambda_1':[1e-05],\n",
    "        'lambda_2':[1e-06],\n",
    "        'alpha_init':[None],\n",
    "        'lambda_init':[None],\n",
    "        'compute_score':[True,False],\n",
    "        'fit_intercept':[True,False],\n",
    "        'normalize':[False,True],\n",
    "        'copy_X':[True],\n",
    "        'verbose':[False]\n",
    "    }\n",
    "\n",
    "scorers = {\n",
    "    'r2': 'r2',\n",
    "    'nmsle': 'neg_mean_squared_log_error',\n",
    "    'nmse': 'neg_mean_squared_error',\n",
    "    'mae': 'neg_mean_absolute_error'\n",
    "}\n",
    "# To be used within GridSearch \n",
    "inner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n",
    "\n",
    "# To be used in outer CV \n",
    "outer_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n",
    "\n",
    "#inner loop KFold example:\n",
    "gsc = GridSearchCV(\n",
    "    estimator=br,\n",
    "    param_grid=param_grid,\n",
    "    #scoring='neg_mean_squared_error', # 'roc_auc', # or 'r2', etc\n",
    "    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n",
    "    #scoring='neg_mean_squared_error', # or look here for other choices \n",
    "    # https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    #cv=5,\n",
    "    cv=inner_cv, # this will use KFold splitting to change train/test/validation datasets randomly\n",
    "    verbose=0,\n",
    "    return_train_score=True, # keep the other scores\n",
    "    refit='nmse' # use this one for optimizing\n",
    ")\n",
    "\n",
    "grid_result = runGSCV(2, train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_br = 0\n",
    "BR = BayesianRidge()\n",
    "if (tuning_br == 1):\n",
    "    BR_new = BayesianRidge(**grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = result_details(grid_result,'alpha_1',100)\n",
    "rd[['alpha_1','alpha_2','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (tuning_br == 1):\n",
    "    model_results = [] # model flow, mae, rmsle\n",
    "    models = [BR, BR_new]\n",
    "\n",
    "    for model in models:\n",
    "        #print(model)\n",
    "        with MyTimer(): \n",
    "            scores = calc_all_scores(model,10,10)\n",
    "        #print(\"------------------------------------------\")\n",
    "        model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n",
    "\n",
    "    df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n",
    "    df_mr.sort_values(by=['rmsle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More Models :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the algorithm for the GridSearchCV function\n",
    "# defaults are best\n",
    "ET = ExtraTreesRegressor()\n",
    "tuningET = 0 # this took 2 hours last time, 1 hour per iteration\n",
    "\n",
    "if (tuningET == 1):\n",
    "    # use this when tuning\n",
    "    param_grid={\n",
    "        'n_estimators':[100], # revisit at possibly 2000, but this algorithm becomes really slow for large values\n",
    "        'criterion':['mse'], # done Default: mse, mae\n",
    "        'max_depth':[None], # done - above 30 result converges to None\n",
    "        'min_samples_split':[2], # done - inconsistently better\n",
    "        'min_samples_leaf':[1],\n",
    "        'min_weight_fraction_leaf':[0.0],\n",
    "        'max_features':['auto'], # done - Default:“auto”, “sqrt”, “log2”, None\n",
    "        'max_leaf_nodes':[None],\n",
    "        'min_impurity_decrease':[0.0],\n",
    "        'min_impurity_split':[None],\n",
    "        'bootstrap':[False],\n",
    "        'oob_score':[False], # done - True doesn't work, results in a nan value\n",
    "        'n_jobs':[None],\n",
    "        'random_state':[1,5,42,55,98],\n",
    "        'verbose':[0],\n",
    "        'warm_start':[False],\n",
    "        'ccp_alpha':[0.0],\n",
    "        'max_samples':[None] # done - no difference\n",
    "    }\n",
    "\n",
    "else:\n",
    "    # use this when not tuning\n",
    "    param_grid={\n",
    "        'n_estimators':[100],\n",
    "        'criterion':['mse'],\n",
    "        'max_depth':[None],\n",
    "        'min_samples_split':[2],\n",
    "        'min_samples_leaf':[1],\n",
    "        'min_weight_fraction_leaf':[0.0],\n",
    "        'max_features':['auto'],\n",
    "        'max_leaf_nodes':[None],\n",
    "        'min_impurity_decrease':[0.0],\n",
    "        'min_impurity_split':[None],\n",
    "        'bootstrap':[False],\n",
    "        'oob_score':[False],\n",
    "        'n_jobs':[None],\n",
    "        'random_state':[None],\n",
    "        'verbose':[0],\n",
    "        'warm_start':[False],\n",
    "        'ccp_alpha':[0.0],\n",
    "        'max_samples':[None]\n",
    "    }\n",
    "\n",
    "scorers = {\n",
    "    'r2': 'r2',\n",
    "    'nmsle': 'neg_mean_squared_log_error',\n",
    "    'nmse': 'neg_mean_squared_error',\n",
    "    'mae': 'neg_mean_absolute_error'\n",
    "}\n",
    "# To be used within GridSearch \n",
    "inner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n",
    "\n",
    "# To be used in outer CV \n",
    "outer_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n",
    "\n",
    "#inner loop KFold example:\n",
    "gsc = GridSearchCV(\n",
    "    estimator=ET,\n",
    "    param_grid=param_grid,\n",
    "    #scoring='neg_mean_squared_error', # 'roc_auc', # or 'r2', etc\n",
    "    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n",
    "    #scoring='neg_mean_squared_error', # or look here for other choices \n",
    "    # https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    #cv=5,\n",
    "    cv=inner_cv, # this will use KFold splitting to change train/test/validation datasets randomly\n",
    "    verbose=0,\n",
    "    return_train_score=True, # keep the other scores\n",
    "    refit='nmse' # use this one for optimizing\n",
    ")\n",
    "\n",
    "grid_result = runGSCV(2, train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = result_details(grid_result,'mean_test_nmse',100)\n",
    "rd[['random_state','n_estimators','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ET = make_pipeline(RobustScaler(), ExtraTreesRegressor()) # was 1 Tree algorithms don't need scaling\n",
    "tuning_et = 0\n",
    "if (tuning_et == 1):\n",
    "    for i in [2,5,20,42,99]:\n",
    "        print('random_state =',i)\n",
    "        ET = ExtraTreesRegressor(random_state=i)\n",
    "        #ET2 = make_pipeline(StandardScaler(), ExtraTreesRegressor(random_state=i))\n",
    "\n",
    "        e = {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'mse', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 6, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 50, 'n_jobs': None, 'oob_score': False, 'verbose': 0, 'warm_start': False}\n",
    "        ET_new = ExtraTreesRegressor(**e, random_state=i)\n",
    "        #ET_new2 = make_pipeline(StandardScaler(), ExtraTreesRegressor(**e, random_state=i))\n",
    "\n",
    "        model_results = [] # model flow, mae, rmsle\n",
    "        models = [ET, ET_new]\n",
    "\n",
    "        for model in models:\n",
    "            #print(model)\n",
    "            with MyTimer(): \n",
    "                scores = calc_all_scores(model,5,5)\n",
    "            #print(\"------------------------------------------\")\n",
    "            model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n",
    "\n",
    "        df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n",
    "        print(df_mr.sort_values(by=['rmsle']))\n",
    "else:\n",
    "    ET_new = make_pipeline(RobustScaler(), ExtraTreesRegressor(**grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the algorithm for the GridSearchCV function\n",
    "R = Ridge(alpha=1.0)\n",
    "tuningR = 1 # this took 2 hours last time, 1 hour per iteration\n",
    "\n",
    "if (tuningR == 1):\n",
    "    # use this when tuning\n",
    "    param_grid={\n",
    "        'alpha':[8], # done\n",
    "        'fit_intercept':[True], # done\n",
    "        'normalize':[False], # done\n",
    "        'copy_X':[True],\n",
    "        'max_iter':[None], # done - no difference\n",
    "        'tol':[0.001],\n",
    "        'solver':['auto'], # done - Default:‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’, ‘sag’, ‘saga’\n",
    "        'random_state':[1,10,42,99,127]\n",
    "    }\n",
    "\n",
    "else:\n",
    "    # use this when not tuning\n",
    "    param_grid={\n",
    "        'alpha':[1.0],\n",
    "        'fit_intercept':[True],\n",
    "        'normalize':[False],\n",
    "        'copy_X':[True],\n",
    "        'max_iter':[None],\n",
    "        'tol':[0.001],\n",
    "        'solver':['auto'],\n",
    "        'random_state':[None]\n",
    "    }\n",
    "\n",
    "scorers = {\n",
    "    'r2': 'r2',\n",
    "    'nmsle': 'neg_mean_squared_log_error',\n",
    "    'nmse': 'neg_mean_squared_error',\n",
    "    'mae': 'neg_mean_absolute_error'\n",
    "}\n",
    "# To be used within GridSearch \n",
    "inner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n",
    "\n",
    "# To be used in outer CV \n",
    "outer_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n",
    "\n",
    "#inner loop KFold example:\n",
    "gsc = GridSearchCV(\n",
    "    estimator=R,\n",
    "    param_grid=param_grid,\n",
    "    #scoring='neg_mean_squared_error', # 'roc_auc', # or 'r2', etc\n",
    "    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n",
    "    #scoring='neg_mean_squared_error', # or look here for other choices \n",
    "    # https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    #cv=5,\n",
    "    cv=inner_cv, # this will use KFold splitting to change train/test/validation datasets randomly\n",
    "    verbose=0,\n",
    "    return_train_score=True, # keep the other scores\n",
    "    refit='nmse' # use this one for optimizing\n",
    ")\n",
    "\n",
    "grid_result = runGSCV(5, train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = result_details(grid_result,'mean_test_nmse',100)\n",
    "summary = rd[['alpha','random_state','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']].sort_values(by=['random_state','mean_test_nmse'])\n",
    "#summary.groupby(['fit_intercept'], as_index=False).agg({'mean_test_nmse': 'mean', 'mean_test_nmse': 'std', 'mean_test_mae': 'mean', 'mean_test_r2': 'mean'}).sort_values(by=['mean_test_mae'])\n",
    "summary.groupby(['alpha'], as_index=False).agg({'mean_test_nmse': 'mean', 'mean_test_mae': 'mean', 'mean_test_r2': 'mean'}).sort_values(by=['mean_test_mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R = make_pipeline(RobustScaler(), Ridge(alpha =0.0005, random_state=1)) # was 1\n",
    "tuning_r = 1\n",
    "if (tuning_r == 1):\n",
    "    for i in [2,5,20,42,99]:\n",
    "        print('random_state =',i)\n",
    "\n",
    "        r= {'alpha': 8, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'solver': 'auto', 'tol': 0.001}\n",
    "        #R_new = make_pipeline(RobustScaler(), Ridge(**r, random_state=i))\n",
    "        R_new = Ridge(**r, random_state=i)\n",
    "\n",
    "        model_results = [] # model flow, mae, rmsle\n",
    "        models = [R, R_new]\n",
    "\n",
    "        for model in models:\n",
    "            #print(model)\n",
    "            with MyTimer(): \n",
    "                scores = calc_all_scores(model,5,5)\n",
    "            #print(\"------------------------------------------\")\n",
    "            model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n",
    "\n",
    "        df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n",
    "        print(df_mr.sort_values(by=['rmsle']))\n",
    "else:\n",
    "    #R_new = make_pipeline(RobustScaler(), Ridge(**grid_result.best_params_))\n",
    "    R_new = Ridge(**grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AB = AdaBoostRegressor()\n",
    "SVR = SVR()\n",
    "DT = DecisionTreeRegressor()\n",
    "KN = KNeighborsRegressor()\n",
    "B = BaggingRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скоры базовых моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (tuning_gb == 1):\n",
    "    model_results = [] # model flow, mae, rmsle\n",
    "    models = [GBoost, GBoost_orig]#, GBoost] # model_lgb_op, lasso_ns, \n",
    "\n",
    "    for model in models:\n",
    "        #print(model)\n",
    "        with MyTimer(): \n",
    "            scores = calc_all_scores(model)\n",
    "        #print(\"------------------------------------------\")\n",
    "        model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n",
    "\n",
    "    df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n",
    "    df_mr.sort_values(by=['rmsle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculate Metrics: \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models = 0\n",
    "if (compare_models == 1):\n",
    "    model_results = [] # model flow, mae, rmsle\n",
    "    # GBoost is better than GBoost_orig, but has lower final score, think this may be  overfitting\n",
    "    # BR_new has same results, here, but better final score\n",
    "    models = [lasso, lasso_new, ENet, ENet_new, KRR, GBoost_orig, GBoost, model_xgb, model_lgb, BR, ET, ET_new, RF, RF_new, AB, SVR, DT, KN, B, R, R_new] # worse or same: BR_new, model_lgb_op, lasso_ns, model_xgb_new,\n",
    "\n",
    "    for model in models:\n",
    "        #print(model)\n",
    "        with MyTimer(): \n",
    "            scores = calc_all_scores(model)\n",
    "        #print(\"------------------------------------------\")\n",
    "        model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n",
    "\n",
    "    df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n",
    "    df_mr.sort_values(by=['rmsle'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stacking and Ensembling: \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединение моделей\n",
    "\n",
    "Simplest Stacking approach : Averaging base models\n",
    "We begin with this simple approach of averaging base models. We build a new class to extend scikit-learn BaseEstimator, RegressorMixin, TransformerMixin classes with our model and also to leverage encapsulation and code reuse (inheritance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Averaged base models class\n",
    "\n",
    "In averaging methods, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        # IDEA: return weighted means\n",
    "        return np.mean(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Averaged base models score\n",
    "\n",
    "We just average four models here ENet, GBoost, KRR and lasso. Of course we could easily add more models in the mix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AveragingModels will fit and predict each model and predict using the mean of the individual predictions\n",
    "with MyTimer():\n",
    "    averaged_models = AveragingModels(models = (model_lgb, model_xgb, GBoost_orig, KRR, BR)) # Adding ENet and RF is worse, model_xgb_new is worse\n",
    "    #averaged_models = AveragingModels(models = (GBoost))\n",
    "\n",
    "if (competition == 'SR'):\n",
    "    score = mae_cv(averaged_models)\n",
    "    print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "else:\n",
    "    score = rmsle_cv(averaged_models)\n",
    "    print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_models.fit(train.values, y_train)\n",
    "averaged_train_pred = averaged_models.predict(train.values)\n",
    "averaged_pred = inv_boxcox1p(averaged_models.predict(test.values), lam_l)\n",
    "\n",
    "if (competition == 'SR'):\n",
    "    print(mae(y_train, averaged_train_pred))\n",
    "else:\n",
    "    print(rmsle(y_train, averaged_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_average = 0 # AveragingModels Averages\n",
    "#use_average = 1 # VotingRegressor Averages\n",
    "if (use_average == 1):\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "    e = {'alpha': 0.05, 'fit_intercept': True, 'l1_ratio': 0.85, 'max_iter': 100, 'normalize': False, 'selection': 'random'}\n",
    "    r = {'alpha': 8, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'solver': 'auto', 'tol': 0.001, 'cv': None, 'gcv_mode': 'auto', 'random_state': 99} #  \n",
    "    k = {'alpha': 2.2, 'coef0': 0.5, 'degree': 5, 'gamma': 0.001, 'kernel': 'polynomial'}\n",
    "\n",
    "    # ('enet', make_pipeline(StandardScaler(), ElasticNet(**e, random_state=3)))\n",
    "    estimator_list = [('xgb', xgb.XGBRegressor(booster='gbtree',colsample_bytree=0.4603, gamma=0.0468, learning_rate=0.05, max_depth=3, min_child_weight=1.7817, n_estimators=2200, reg_alpha=0.4640, reg_lambda=0.8571, subsample=0.5213, silent=1, random_state=7)),\n",
    "                  ('lgb', lgb.LGBMRegressor(objective='regression',num_leaves=5, learning_rate=0.05, n_estimators=720, max_bin = 55, bagging_fraction = 0.8, bagging_freq = 5, feature_fraction = 0.2319, feature_fraction_seed=9, bagging_seed=9, min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)),\n",
    "                  ('gboost', GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10,  loss='huber', random_state=5)),\n",
    "                  ('krr', KernelRidge(**k)),\n",
    "                  ('br', BayesianRidge())]\n",
    "\n",
    "    ereg = VotingRegressor(estimators=estimator_list, weights=[1,1,1,1,1]) \n",
    "    ereg = ereg.fit(train, y_train)\n",
    "    averaged_pred = inv_boxcox1p(ereg.predict(test), lam_l)\n",
    "    averaged_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less simple Stacking : Adding a Meta-model\n",
    "In this approach, we add a meta-model on averaged base models and use the out-of-folds predictions of these base models to train our meta-model.\n",
    "\n",
    "The procedure, for the training part, may be described as follows:\n",
    "\n",
    "Split the total training set into two disjoint sets (here train and .holdout )\n",
    "\n",
    "Train several base models on the first part (train)\n",
    "\n",
    "Test these base models on the second part (holdout)\n",
    "\n",
    "Use the predictions from 3) (called out-of-folds predictions) as the inputs, and the correct responses (target variable) as the outputs to train a higher level learner called meta-model.\n",
    "\n",
    "The first three steps are done iteratively . If we take for example a 5-fold stacking , we first split the training data into 5 folds. Then we will do 5 iterations. In each iteration, we train every base model on 4 folds and predict on the remaining fold (holdout fold).\n",
    "\n",
    "So, we will be sure, after 5 iterations , that the entire data is used to get out-of-folds predictions that we will then use as new feature to train our meta-model in the step 4.\n",
    "\n",
    "For the prediction part , We average the predictions of all base models on the test data and used them as meta-features on which, the final prediction is done with the meta-model.\n",
    "\n",
    "Another way to combine multiple models:\n",
    "\n",
    "The function cross_val_predict is appropriate for: Visualization of predictions obtained from different models.\n",
    "\n",
    "Model blending: When predictions of one supervised estimator are used to train another estimator in ensemble methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference between M6 and M1-M5 is, that M6 is trained on the entire original training data, whereas M1-M5 are trained only on 4 out of 5 folds.\n",
    "\n",
    "With M1-M5 you can build valid out-of-fold predictions for the training set (the orange ones) to form a \"new feature\" for the 2nd layer (not possible with M6). You can also predict the test set with M1-M5 to get 5 sets of test set predictions .. but you only need one set of test set predictions for the corresponding feature to the orange out-of-fold train set predictions.\n",
    "\n",
    "Hence, you reduce those 5 sets to 1 by averaging. That's the first variant. Alternatively, you train M6 and use its test set predictions as feature for the 2nd layer (instead of the average of the test set predictions from M1-M5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=10): # increasing this value should give a more accurate prediction, averaged over n_fold iterations\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156) # was 156\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models): # for each model passed in\n",
    "            for train_index, holdout_index in kfold.split(X, y): # create train,test for the number of folds\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index]) # fit the model for this fold\n",
    "                y_pred = instance.predict(X[holdout_index]) # predict values for this fold\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred # think we either use all of these values as features later, or the mean value?\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new and only feature\n",
    "        print(\"out_of_fold_predictions\", out_of_fold_predictions)\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y) # need to see out_of_fold_predictions feature set\n",
    "        return self\n",
    "   \n",
    "    # Calculate the predictions of all base models on the test data and use the averaged predictions as \n",
    "    # meta-features for the final prediction which is calculated by the meta-model\n",
    "    def predict(self, X):\n",
    "        # column_stack() function is used to stack 1-D arrays as columns into a 2-D array.\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~ 10 minutes to run\n",
    "\n",
    "#stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR), # adding RF here did not help\n",
    "#                                                 meta_model = lasso)\n",
    "# verify: this class uses out of fold predictions in the stacking method, so rows in dataset are split up betwen models and each row in dataset is only used once\n",
    "stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost_orig, KRR, BR), # adding RF here did not help\n",
    "                                                 meta_model = R_new)\n",
    "\n",
    "if (compare_models == 1):\n",
    "    with MyTimer():\n",
    "        if (competition == 'SR'):\n",
    "            score = mae_cv(stacked_averaged_models)\n",
    "            print(\"Stacking Averaged models score mean and std: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "        else:\n",
    "            score = rmsle_cv(stacked_averaged_models)\n",
    "            print(\"Stacking Averaged models score mean and std: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_averaged_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_Regressor = 0 # default - best score\n",
    "#use_Regressor = 1 # StackingRegressor - worst score\n",
    "#use_Regressor = 2 # StackingCVRegressor - middle score\n",
    "\n",
    "if (use_Regressor == 1):\n",
    "    from sklearn.ensemble import StackingRegressor\n",
    "    from sklearn.linear_model import RidgeCV, LassoCV\n",
    "    from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "    use_cv = 1\n",
    "    k = {'alpha': 2.2, 'coef0': 0.5, 'degree': 5, 'gamma': 0.001, 'kernel': 'polynomial'}\n",
    "    if (use_cv == 1):\n",
    "        e = {'fit_intercept': True, 'l1_ratio': 0.85, 'max_iter': 100, 'normalize': False, 'selection': 'random', 'cv': 10} # 'alpha': 0.05,\n",
    "        r = {'fit_intercept': True, 'normalize': False, 'cv': None, 'gcv_mode': 'auto'} # cv value has no effect\n",
    "        estimators = [('enet', make_pipeline(StandardScaler(), ElasticNetCV(**e, random_state=3))),\n",
    "                      ('gboost', GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10,  loss='huber', random_state=5)),\n",
    "                      ('krr', KernelRidge(**k)),\n",
    "                      ('br', BayesianRidge())]\n",
    "        reg = StackingRegressor(\n",
    "            estimators=estimators,\n",
    "            final_estimator=RidgeCV(**r))\n",
    "    else:\n",
    "        e = {'alpha': 0.05, 'fit_intercept': True, 'l1_ratio': 0.85, 'max_iter': 100, 'normalize': False, 'selection': 'random'}\n",
    "        r = {'alpha': 8, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'solver': 'auto', 'tol': 0.001, 'cv': None, 'gcv_mode': 'auto', 'random_state': 99} #  \n",
    "        estimators = [('enet', make_pipeline(StandardScaler(), ElasticNet(**e, random_state=3))),\n",
    "                      ('gboost', GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10,  loss='huber', random_state=5)),\n",
    "                      ('krr', KernelRidge(**k)),\n",
    "                      ('br', BayesianRidge())]\n",
    "        reg = StackingRegressor(\n",
    "            estimators=estimators,\n",
    "            final_estimator=Ridge(**r))\n",
    "\n",
    "    reg.fit(train, y_train)\n",
    "    stacked_pred = inv_boxcox1p(reg.predict(test.values), lam_l)\n",
    "    #reg.transform(inv_boxcox1p(stacked_averaged_models2, lam_l))\n",
    "    print(stacked_pred)\n",
    "    print(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (use_Regressor == 2):\n",
    "    from mlxtend.regressor import StackingCVRegressor\n",
    "    from sklearn.linear_model import RidgeCV, LassoCV\n",
    "    from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "    use_cv = 0\n",
    "    k = {'alpha': 2.2, 'coef0': 0.5, 'degree': 5, 'gamma': 0.001, 'kernel': 'polynomial'}\n",
    "    if (use_cv == 1):\n",
    "        e = {'fit_intercept': True, 'l1_ratio': 0.85, 'max_iter': 100, 'normalize': False, 'selection': 'random', 'cv': 10} # 'alpha': 0.05,\n",
    "        r = {'fit_intercept': True, 'normalize': False, 'cv': None, 'gcv_mode': 'auto'} # cv value has no effect\n",
    "        enet = make_pipeline(StandardScaler(), ElasticNetCV(**e, random_state=3))\n",
    "        gboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10,  loss='huber', random_state=5)\n",
    "        krr = KernelRidge(**k)\n",
    "        br = BayesianRidge()\n",
    "        r = RidgeCV(**r)\n",
    "        reg = StackingCVRegressor(\n",
    "            regressors=(enet, gboost, krr, br),\n",
    "            meta_regressor=r)\n",
    "    else:\n",
    "        e = {'alpha': 0.05, 'fit_intercept': True, 'l1_ratio': 0.85, 'max_iter': 100, 'normalize': False, 'selection': 'random'}\n",
    "        r = {'alpha': 8, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'solver': 'auto', 'tol': 0.001, 'random_state': 99} #  \n",
    "        enet = make_pipeline(StandardScaler(), ElasticNet(**e, random_state=3))\n",
    "        gboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10,  loss='huber', random_state=5)\n",
    "        krr = KernelRidge(**k)\n",
    "        br = BayesianRidge()\n",
    "        r = Ridge(**r)\n",
    "        reg = StackingCVRegressor(\n",
    "            regressors=(enet, gboost, krr, br),\n",
    "            meta_regressor=r)\n",
    "\n",
    "    reg.fit(train, y_train)\n",
    "    stacked_pred = inv_boxcox1p(reg.predict(test.values), lam_l)\n",
    "    print(stacked_pred)\n",
    "    print(reg)\n",
    "    \n",
    "    print('5-fold cross validation scores:\\n')\n",
    "    for clf, label in zip([enet, gboost, krr, br], ['enet', 'gboost', \n",
    "                                                'krr', 'br',\n",
    "                                                'StackingCVRegressor']):\n",
    "        scores = cross_val_score(clf, train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "        print(\"Neg. MSE Score: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "        \n",
    "        #scores = cross_val_score(clf, train, y_train, cv=5)\n",
    "        #print(\"R^2 Score: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensembling StackedRegressor, XGBoost and LightGBM\n",
    "We add XGBoost and LightGBM to the StackedRegressor defined previously.\n",
    "\n",
    "Final Training and Prediction\n",
    "StackedRegressor:\n",
    "\n",
    "add the previous averaged models here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_models.fit(train.values, y_train)\n",
    "averaged_train_pred = averaged_models.predict(train.values)\n",
    "if (use_average == 0):\n",
    "    averaged_pred = inv_boxcox1p(averaged_models.predict(test.values), lam_l)\n",
    "\n",
    "if (competition == 'SR'):\n",
    "    print(mae(y_train, averaged_train_pred))\n",
    "else:\n",
    "    print(rmsle(y_train, averaged_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(averaged_train_pred, y_train, alpha=.7,\n",
    "            color='b') #alpha helps to show overlapping data\n",
    "plt.xlabel('Predicted Price')\n",
    "plt.ylabel('Actual Price')\n",
    "plt.title('Averaged Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_models.predict(test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_averaged_models.fit(train.values, y_train)\n",
    "stacked_train_pred = stacked_averaged_models.predict(train.values)\n",
    "if (use_Regressor == 0):\n",
    "    stacked_pred = inv_boxcox1p(stacked_averaged_models.predict(test.values), lam_l)\n",
    "    \n",
    "with MyTimer():\n",
    "    if (competition == 'SR'):\n",
    "        print(mae(y_train, stacked_train_pred))\n",
    "    else:\n",
    "        print(rmsle(y_train, stacked_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pred(train, ytrain, test, model):\n",
    "        model.fit(train, y_train)\n",
    "        model_train_pred = model.predict(train)\n",
    "        model_pred = inv_boxcox1p(model.predict(test), lam_l)\n",
    "        return(model_train_pred, model_pred)\n",
    "    \n",
    "models = [lasso, lasso_new, ENet, KRR, GBoost_orig, GBoost, model_xgb, model_xgb_new, BR, ET, ET_new, RF, RF_new, AB, SVR, DT, KN, B] # model_lgb,\n",
    "model_names = ['lasso', 'lasso_new', 'ENet', 'KRR', 'GBoost_orig', 'GBoost', 'model_xgb', 'model_xgb_new', 'BR', 'ET', 'ET_new', 'RF', 'RF_new', 'AB', 'SVR', 'DT', 'KN', 'B']\n",
    "\n",
    "with MyTimer():\n",
    "    for i in range(0,len(models)):\n",
    "        mn = model_names[i]+\"_pred\"\n",
    "        train_pred, test_pred = fit_pred(train, y_train, test, models[i])\n",
    "        print(mn, test_pred)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are train and test normalized? between -1 and 1\n",
    "replace_xgb = 0 # new optimized model is worse, was overfit\n",
    "if (replace_xgb == 1):\n",
    "    model_xgb_new.fit(train, y_train)\n",
    "    xgb_train_pred = model_xgb_new.predict(train)\n",
    "    xgb_pred = inv_boxcox1p(model_xgb_new.predict(test), lam_l)\n",
    "else:\n",
    "    model_xgb.fit(train, y_train)\n",
    "    xgb_train_pred = model_xgb.predict(train)\n",
    "    xgb_pred = inv_boxcox1p(model_xgb.predict(test), lam_l)\n",
    "\n",
    "if (competition == 'SR'):\n",
    "    print(mae(y_train, xgb_train_pred))\n",
    "else:\n",
    "    print(rmsle(y_train, xgb_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb.fit(train, y_train)\n",
    "lgb_train_pred = model_lgb.predict(train)\n",
    "lgb_pred = inv_boxcox1p(model_lgb.predict(test.values), lam_l)\n",
    "\n",
    "if (competition == 'SR'):\n",
    "    print(mae(y_train, lgb_train_pred))\n",
    "else:\n",
    "    print(rmsle(y_train, lgb_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (tuning_lgb == 1):\n",
    "    model_lgb_op.fit(train, y_train)\n",
    "    lgb_train_pred_op = model_lgb_op.predict(train)\n",
    "    lgb_pred_op = inv_boxcox1p(model_lgb_op.predict(test.values), lam_l)\n",
    "\n",
    "    if (competition == 'SR'):\n",
    "        print(mae(y_train, lgb_train_pred_op))\n",
    "    else:\n",
    "        print(rmsle(y_train, lgb_train_pred_op))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare values with optimization\n",
    "print(lgb_train_pred)\n",
    "if (tuning_lgb == 1):\n",
    "    print(lgb_train_pred_op)\n",
    "print(lgb_pred)\n",
    "if (tuning_lgb == 1):\n",
    "    print(lgb_pred_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''RMSE on the entire Train data when averaging'''\n",
    "\n",
    "print('RMSLE score on train data:')\n",
    "\n",
    "\n",
    "stk = 0.70\n",
    "xgb = 0.15\n",
    "lgb = 0.15\n",
    "\n",
    "\n",
    "if (competition == 'SR'):\n",
    "    print(mae(y_train,stacked_train_pred*stk + xgb_train_pred*xgb + lgb_train_pred*lgb ))\n",
    "else:\n",
    "    print(rmsle(y_train,stacked_train_pred*stk + xgb_train_pred*xgb + lgb_train_pred*lgb ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble prediction:\n",
    "\n",
    "when deciding which models to include in an ensemble:\n",
    "fewer are better\n",
    "more diverse are better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method = 'ensemble'\n",
    "method = 'stacked'\n",
    "if (method == 'stacked'):\n",
    "    #stacked_pred => ENet, GBoost_orig, KRR\n",
    "    ensemble = stacked_pred*stk + xgb_pred*xgb + lgb_pred*lgb  # if using averaged_pred, need to add averaged_pred here\n",
    "else:\n",
    "    ensemble = averaged_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lgb_pred)\n",
    "print(xgb_pred)\n",
    "print(stacked_pred)\n",
    "print(averaged_pred)\n",
    "print(ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train,stacked_train_pred * stk + xgb_train_pred * xgb + lgb_train_pred * lgb) # if using averaged_pred, need to add averaged_pred here\n",
    "print(y_train,stacked_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train = pd.DataFrame()\n",
    "sub_train['Id'] = train_ID\n",
    "sub_train['SalePrice'] = inv_boxcox1p(stacked_train_pred, lam_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predicted = sub_train['SalePrice']\n",
    "Actual = inv_boxcox1p(y_train, lam_l)\n",
    "plt.scatter(sub_train['SalePrice'], Actual, alpha=.7,\n",
    "            color='b') #alpha helps to show overlapping data\n",
    "plt.xlabel('Predicted Price')\n",
    "plt.ylabel('Actual Price')\n",
    "plt.title('Averaged Model')\n",
    "\n",
    "m, b = np.polyfit(Predicted, Actual, 1)\n",
    "#m = slope, b=intercept\n",
    "plt.plot(Predicted, m*Predicted+b,c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the high end, SP > 520000, the model predicts too low for 7/8 points\n",
    "\n",
    "Get Pre-adjustment score for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-adjustment score\n",
    "print(\"mae for boxcox(SalePrice)\",mae(y_train, sub_train['SalePrice']))\n",
    "print(\"mse for boxcox(SalePrice)\",rmsle(y_train, sub_train['SalePrice']))\n",
    "print(\"mae for SalePrice\",mae(Actual, Predicted))\n",
    "print(\"mse for SalePrice\",rmsle(Actual, Predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdjustedScores = []\n",
    "\n",
    "for i in np.arange(.994, .996, 0.001):\n",
    "    for j in np.arange(1.06, 1.08, .01):\n",
    "\n",
    "        q1 = sub_train['SalePrice'].quantile(0.0025)\n",
    "        q2 = sub_train['SalePrice'].quantile(0.0045)\n",
    "        q3 = sub_train['SalePrice'].quantile(i)\n",
    "\n",
    "        #Verify the cutoffs for the adjustment\n",
    "        print(q1,q2,q3)\n",
    "        # adjust at low end\n",
    "        #sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x > q1 else x*0.79)\n",
    "        #sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x > q2 else x*0.89)\n",
    "\n",
    "        # adjust at high end\n",
    "        sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x < q3 else x*j)\n",
    "\n",
    "        Predicted = sub_train['SalePrice2']\n",
    "        Actual = inv_boxcox1p(y_train, lam_l)\n",
    "\n",
    "        # Pre-adjustment score\n",
    "        print(\"mae for boxcox(SalePrice)\",mae(y_train, sub_train['SalePrice2']))\n",
    "        print(\"mse for boxcox(SalePrice)\",rmsle(y_train, sub_train['SalePrice2']))\n",
    "        print(\"mae for SalePrice\",mae(Actual, Predicted))\n",
    "        print(\"mse for SalePrice\",rmsle(Actual, Predicted))\n",
    "\n",
    "        AdjustedScores.append([i, j, mae(y_train, boxcox1p(sub_train['SalePrice2'], lam_l)), rmsle(y_train, boxcox1p(sub_train['SalePrice2'], lam_l)), mae(Actual, Predicted), rmsle(Actual, Predicted)])\n",
    "\n",
    "df_adj = pd.DataFrame(AdjustedScores, columns=[\"QUANT\",\"COEF\",\"MAE_BC\",\"RMSE_BC\",\"MAE_SP\",\"RMSE_SP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = sub_train['SalePrice'].quantile(0.0015)\n",
    "q2 = sub_train['SalePrice'].quantile(0.01)\n",
    "q3 = sub_train['SalePrice'].quantile(0.995)\n",
    "\n",
    "#Verify the cutoffs for the adjustment\n",
    "print(q1,q2,q3)\n",
    "# adjust at low end\n",
    "#sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x > q1 else x*0.79)\n",
    "#sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x > q2 else x*0.89)\n",
    "\n",
    "# adjust at high end\n",
    "sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x < q3 else x*1.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sub_train['SalePrice'], sub_train['SalePrice2'], alpha=.7,\n",
    "            color='b') #alpha helps to show overlapping data\n",
    "plt.xlabel('Predicted Price')\n",
    "plt.ylabel('Adjusted Predicted Price')\n",
    "plt.title('Averaged Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train.query(\"SalePrice != SalePrice2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predicted = sub_train['SalePrice2']\n",
    "Actual = inv_boxcox1p(y_train, lam_l)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.scatter(sub_train['SalePrice2'], Actual, alpha=.7,\n",
    "            color='b') #alpha helps to show overlapping data\n",
    "plt.xlabel('Adj Predicted Price')\n",
    "plt.ylabel('Actual Price')\n",
    "plt.title('Averaged Model')\n",
    "\n",
    "m, b = np.polyfit(Predicted, Actual, 1)\n",
    "#m = slope, b=intercept\n",
    "plt.plot(Predicted, m*Predicted+b,c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post adjustment for high score\n",
    "print(\"mae for boxcox(SalePrice2)\",mae(y_train, sub_train['SalePrice2']))\n",
    "print(\"mse for boxcox(SalePrice2)\",rmsle(y_train, sub_train['SalePrice2']))\n",
    "print(\"mae for SalePrice2\",mae(Actual, Predicted))\n",
    "print(\"mse for SalePrice2\",rmsle(Actual, Predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub_train['Id'] = train_ID\n",
    "#sub_train['SalePrice'] = inv_boxcox1p(stacked_train_pred, lam_l)\n",
    "q1 = sub_train['SalePrice'].quantile(0.013)\n",
    "q2 = sub_train['SalePrice'].quantile(0.10)\n",
    "q3 = sub_train['SalePrice'].quantile(0.995)\n",
    "print(q1,q2,q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdjustedScores = []\n",
    "\n",
    "for i in np.arange(.005, .015, 0.001):\n",
    "    for j in np.arange(.90, 1.00, 0.01):\n",
    "\n",
    "        q1 = sub_train['SalePrice'].quantile(i)\n",
    "        q2 = sub_train['SalePrice'].quantile(0.1)\n",
    "        q3 = sub_train['SalePrice'].quantile(.995)\n",
    "\n",
    "        #Verify the cutoffs for the adjustment\n",
    "        print(q1,q2,q3)\n",
    "        # adjust at low end\n",
    "        sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x > q1 else x*j)\n",
    "        #sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x > q2 else x*0.89)\n",
    "\n",
    "        # adjust at high end\n",
    "        #sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x < q3 else x*j)\n",
    "\n",
    "        Predicted = sub_train['SalePrice2']\n",
    "        Actual = inv_boxcox1p(y_train, lam_l)\n",
    "\n",
    "        # Pre-adjustment score\n",
    "        print(\"mae for boxcox(SalePrice)\",mae(y_train, sub_train['SalePrice2']))\n",
    "        print(\"mse for boxcox(SalePrice)\",rmsle(y_train, sub_train['SalePrice2']))\n",
    "        print(\"mae for SalePrice\",mae(Actual, Predicted))\n",
    "        print(\"mse for SalePrice\",rmsle(Actual, Predicted))\n",
    "\n",
    "        AdjustedScores.append([i, j, mae(y_train, boxcox1p(sub_train['SalePrice2'], lam_l)), rmsle(y_train, boxcox1p(sub_train['SalePrice2'], lam_l)), mae(Actual, Predicted), rmsle(Actual, Predicted)])\n",
    "\n",
    "df_adj = pd.DataFrame(AdjustedScores, columns=[\"QUANT\",\"COEF\",\"MAE_BC\",\"RMSE_BC\",\"MAE_SP\",\"RMSE_SP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adj.sort_values(by=['RMSE_BC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = sub_train['SalePrice'].quantile(0.01)\n",
    "q2 = sub_train['SalePrice'].quantile(0.1)\n",
    "q3 = sub_train['SalePrice'].quantile(0.995)\n",
    "\n",
    "#Verify the cutoffs for the adjustment\n",
    "print(q1,q2,q3)\n",
    "# adjust at low end\n",
    "sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x > q1 else x*0.91) # also try .94\n",
    "#sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x > q2 else x*0.89)\n",
    "\n",
    "# adjust at high end\n",
    "sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x < q3 else x*1.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.scatter(sub_train['SalePrice'], sub_train['SalePrice2'], alpha=.7,\n",
    "            color='b') #alpha helps to show overlapping data\n",
    "plt.xlabel('Predicted Price')\n",
    "plt.ylabel('Adjusted Predicted Price')\n",
    "plt.title('Averaged Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predicted = sub_train['SalePrice2']\n",
    "Actual = inv_boxcox1p(y_train, lam_l)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.scatter(sub_train['SalePrice2'], Actual, alpha=.7,\n",
    "            color='b') #alpha helps to show overlapping data\n",
    "plt.xlabel('Adj Predicted Price')\n",
    "plt.ylabel('Actual Price')\n",
    "plt.title('Averaged Model')\n",
    "\n",
    "m, b = np.polyfit(Predicted, Actual, 1)\n",
    "#m = slope, b=intercept\n",
    "plt.plot(Predicted, m*Predicted+b,c='red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post adjustment for low and high score\n",
    "print(\"mae for boxcox(SalePrice2)\",mae(y_train, sub_train['SalePrice2']))\n",
    "print(\"mse for boxcox(SalePrice2)\",rmsle(y_train, sub_train['SalePrice2']))\n",
    "print(\"mae for SalePrice2\",mae(Actual, Predicted))\n",
    "print(\"mse for SalePrice2\",rmsle(Actual, Predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['Id'] = test_ID\n",
    "sub['SalePrice'] = ensemble\n",
    "sub.to_csv('Output//Accurate_EDA_solution_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = sub['SalePrice'].quantile(0.01)\n",
    "q2 = sub['SalePrice'].quantile(0.1)\n",
    "q3 = sub['SalePrice'].quantile(0.995)\n",
    "\n",
    "# adjust at low end\n",
    "#sub['SalePrice'] = sub['SalePrice'].apply(lambda x: x if x > q1 else x*0.99) # didn't help, with several values\n",
    "#sub['SalePrice2'] = sub['SalePrice'].apply(lambda x: x if x > q2 else x*0.89)\n",
    "\n",
    "# adjust at high end\n",
    "sub['SalePrice'] = sub['SalePrice'].apply(lambda x: x if x < q3 else x*1.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('Output//Accurate_EDA_solution_submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
